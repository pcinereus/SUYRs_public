---
title: "Bayesian GAM Part1"
author: "Murray Logan"
date: today
date-format: "DD/MM/YYYY"
format: 
  html:
    ## Format
    theme: [default, ../public/resources/ws-style.scss]
    css: ../public/resources/ws_style.css
    html-math-method: mathjax
    ## Table of contents
    toc: true
    toc-float: true
    ## Numbering
    number-sections: true
    number-depth: 3
    ## Layout
    page-layout: full
    fig-caption-location: "bottom"
    fig-align: "center"
    fig-width: 4
    fig-height: 4
    fig-dpi: 72
    tbl-cap-location: top
    ## Code
    code-fold: false
    code-tools: true
    code-summary: "Show the code"
    code-line-numbers: true
    code-block-border-left: "#ccc"
    code-copy: true
    highlight-style: atom-one
    ## Execution
    execute:
      echo: true
      cache: true
    ## Rendering
    embed-resources: true
crossref:
  fig-title: '**Figure**'
  fig-labels: arabic
  tbl-title: '**Table**'
  tbl-labels: arabic
engine: knitr
output_dir: "docs"
documentclass: article
fontsize: 12pt
mainfont: Arial
mathfont: LiberationMono
monofont: DejaVu Sans Mono
classoption: a4paper
bibliography: ../public/resources/references.bib
---

```{r setup, include=FALSE, warnings=FALSE, message=FALSE}
knitr::opts_chunk$set(cache.lazy = FALSE, tidy='styler')
```
 
# Preparations
 
Load the necessary libraries

```{r}
#| label: libraries
#| output: false
#| eval: true
#| warning: false
#| message: false
#| cache: false

library(tidyverse)  #for data wrangling etc
library(cmdstanr)   #for cmdstan
library(brms)       #for fitting models in STAN
library(standist)   #for exploring distributions
library(coda)       #for diagnostics
library(bayesplot)  #for diagnostics
library(ggmcmc)     #for MCMC diagnostics
library(DHARMa)     #for residual diagnostics
library(rstan)      #for interfacing with STAN
library(emmeans)    #for marginal means etc
library(broom)      #for tidying outputs
library(tidybayes)  #for more tidying outputs
library(HDInterval) #for HPD intervals
library(ggeffects)  #for partial plots
library(broom.mixed)#for summarising models
library(posterior)  #for posterior draws
library(ggeffects)  #for partial effects plots
library(patchwork)  #for multi-panel figures
library(bayestestR) #for ROPE
library(see)        #for some plots
library(easystats)     #framework for stats, modelling and visualisation
library(mgcv)
library(gratia)
theme_set(theme_grey()) #put the default ggplot theme back
source('helperFunctions.R')
```



# Scenario

This is an entirely fabricated example (how embarrising).
So here is a picture of some Red Grouse Chicks to compensate..

![Red grouse chicks](../public/resources/redgrousechicks.jpg){#fig-grouse width="251" height="290"}


x  y
-- --
2  3
4  5
8  6
10 7
14 4

: Format of data.gp.csv data file {#tbl-data.gp .table-condensed}

------    -----------------------------
**x**     - a continuous predictor
**y**     - a continuous response
------    -----------------------------

: Format of data.gp.csv data file {#tbl-data.gp1 .table-condensed}

# Read in the data

```{r readData, results='markdown', eval=TRUE}
data_gam <- read_csv("../public/data/data_gam.csv", trim_ws = TRUE)
```

<!-- START_PRIVATE-->
::: {.panel-tabset}

## glimpse
```{r}
#| label: examinData
#| dependson: readData

data_gam |> glimpse()
```

## head
```{r}
#| label: headData
#| dependson: readData
## Explore the first 6 rows of the data
data_gam |> head()
```

## str
```{r}
#| label: strData
#| dependson: readData
data_gam |> str()
```

## Easystats (datawizard)

```{r}
#| label: easyData
#| dependson: readData
data_gam |> datawizard::data_codebook()
```
:::

<!-- END_PRIVATE-->


# Exploratory data analysis

Model formula:
$$
\begin{align}
y_i &\sim{} \mathcal{N}(\mu_i, \sigma^2)\\
\mu_i &=\beta_0 + f(x_i)\\
f(x_i) &= \sum^k_{j=1}{b_j(x_i)\beta_j}
\end{align}
$$

where $\beta_0$ is the y-intercept, and $f(x)$ indicates an additive smoothing function of $x$. 

<!-- START_PRIVATE-->
Although this is a ficticious example without a clear backstory, given that
there are two continous predictors (and that one of these has been identified as
a response and the other a predictor), we can assume that we might be interested
in investigating the relationship between the two.
As such, our typically starting point is to explore the basic trend between the
two using a scatterplot.

```{r EDA1a, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=4, fig.height=4}
ggplot(data_gam, aes(y=y, x=x))+ 
    geom_point()+
    geom_line()
```

This does not look like a particularly linear relationship.  Lets fit a loess smoother..

```{r EDA1b, results='markdown', eval=TRUE, mhidden=TRUE,warning=FALSE,message=FALSE, fig.width=4, fig.height=4}
ggplot(data_gam, aes(y=y, x=x))+ 
    geom_point()+
    geom_smooth()
```

And what would a linear smoother look like?

```{r EDA1c, results='markdown', eval=TRUE, mhidden=TRUE,warning=FALSE,message=FALSE, fig.width=4, fig.height=4}
ggplot(data_gam, aes(y=y, x=x))+ 
    geom_point()+
    geom_smooth(method='lm')
```

Rather than either a loess or linear smoother, we can also try a Generalized
Additive Model (GAM) smoother. Dont pay too much attention to the GAM formula at
this stage, this will be discussed later in the Model Fitting section.

```{r EDA1d, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=4, fig.height=4}
ggplot(data_gam, aes(y=y, x=x))+ 
    geom_point()+
    geom_smooth(method='gam', formula=y~s(x,k=3))
```

**Conclusions:**

- it is clear that the relationship is not linear.
- it does appear that as x inreases, y initially increases before eventually
  declining again.
- we could model this with a polynomial, but for this exemple, we will use these
  data to illustrate the fitting of GAMs.

<!-- END_PRIVATE-->

# Fit the model

<!-- START_PRIVATE-->
::: {.panel-tabset}
## basis functions
Prior to fitting the GAM, it might be worth gaining a bit of an understanding of
what will occur behind the scenes.

Lets say we intended to fit a smoother with three knots.  The three knots equate
to one at each end of the trend and one in the middle.  We could reexpress our
predictor (x) as three dummy variables that collectively reflect a spline (in
this case, potentially two joined polynomials).


```{r fitModel1, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=4, fig.height=4}
data.frame(smoothCon(s(x, k=3),  data=data_gam)[[1]]$X) %>%
  bind_cols(data_gam)
```

And we could visualize these dummies as three separate components.

```{r fitModel2, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=4, fig.height=4}
basis(s(x, k=3),  data=data_gam) %>% draw()
basis(s(x, k=3, bs='cr'),  data=data_gam) %>% draw()
```

OR

```{r fitModel3, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=4, fig.height=4}
newdata <-
    data.frame(smoothCon(s(x, k=3),  data=data_gam)[[1]]$X) %>%
    bind_cols(data_gam)
ggplot(newdata,  aes(x=x)) +
    geom_line(aes(y=X1)) +
    geom_line(aes(y=X2)) +
    geom_line(aes(y=X3))
```

## brms

brms follows the same basic process as `gamm4`. That is the smooths
are partitioned into two components:

- a penalised component which is treated as a random effect
- an unpenalised component that is treated as a fixed effect

The wiggliness penalty matrix is the precision matrix when 
the smooth is treated as a random effect
The smoothness of a term is determined by estimating the variance of the term

:::: {.panel-tabset}

https://mc-stan.org/cmdstanr/articles/cmdstanr.html
    
### Using default priors

In `brms`, the default priors are designed to be weakly informative.  They are
chosen to provide moderate regularisation (to help prevent over-fitting) and
help stabilise the computations.

Unlike `rstanarm`, `brms` models must be compiled before they start sampling.
For most models, the compilation of the stan code takes around 45 seconds.

```{r fitModel1a, results='markdown', eval=TRUE, mhidden=TRUE, cache=TRUE, error = TRUE}
data_gam.form <- bf(y ~ s(x), family = gaussian())
data_gam.brm <- brm(data_gam.form,
               data = data_gam,
               iter = 5000,
               warmup = 1000,
               chains = 3,
               thin = 5,
               refresh = 0,
               backend = 'cmdstan')
```

```{r fitModel1b, results='markdown', eval=TRUE, mhidden=TRUE, cache=TRUE}
data_gam.form <- bf(y ~ s(x, k = 3), family = gaussian())
get_prior(data_gam.form, data =  data_gam)
data_gam.brm <- brm(data_gam.form,
               data = data_gam,
               iter = 5000,
               warmup = 1000,
               chains = 3, cores = 3,
               thin = 5,
               refresh = 0,
               backend = 'cmdstan')
```
```{r fitModel1c, results='markdown', eval=TRUE, mhidden=TRUE, paged.print=FALSE,tidy.opts = list(width.cutoff = 80), echo=2}
options(width=100)
prior_summary(data_gam.brm)
options(width=80)
```

sds - standard devation of the wiggly basis function

### Assessing priors 

```{r fitModel1d, results='markdown', eval=TRUE, mhidden=TRUE, cache=TRUE}
data_gam.brm <- brm(data_gam.form,
                 data = data_gam,
                 prior = prior(normal(0, 2.5), class = 'b'), 
                 sample_prior = 'only', 
                 iter = 5000,
                 warmup = 1000,
                 chains = 3,
                 thin = 5,
                 backend = 'cmdstan',
                 refresh = 0)
```

#### conditional_effects

```{r fitModel1d2, results='markdown', eval=TRUE, mhidden=TRUE}
data_gam.brm |> conditional_effects() |>  plot(points=TRUE)
```

### Defining priors 


The following link provides some guidance about defining priors.
[https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations]

When defining our own priors, we typically do not want them to be scaled.

If we wanted to define our own priors that were less vague, yet still
not likely to bias the outcomes, we could try the following priors
(which I have mainly plucked out of thin air):

- $\beta_0$: normal centred at 164 with a standard deviation of 65
  - mean of 164: since `median(fert$YIELD)`
  - sd pf 65: since `mad(fert$YIELD)`
- $\beta_1$: normal centred at 0 with a standard deviation of 2.5
  - sd of 2.5: since `2.5*(mad(fert$YIELD)/mad(fert$FERTILIZER))`
- $\sigma$: half-t centred at 0 with a standard deviation of 65 OR
  - sd pf 65: since `mad(fert$YIELD)`
- $\sigma$: gamma with shape parameters of 2 and 1


#### Sample prior only

I will also overlay the raw data for comparison.

```{r fitModel2a, results='markdown', eval=TRUE, mhidden=TRUE, cache=TRUE}
data_gam |> summarise(median(y), mad(y))
priors <- prior(normal(5, 1.5),  class='Intercept') +
  prior(normal(0, 1.5), class='b') +
  prior(student_t(3, 0, 1.5),  class='sigma') +
  prior(student_t(3, 0, 10), class =  "sds")

data_gam.form <- bf(y ~ s(x, k = 3))
data_gam.brm2 <- brm(data_gam.form,
                 data = data_gam,
                 prior = priors, 
                 sample_prior = 'only', 
                 iter = 5000,
                 warmup = 1000,
                 chains = 3, cores =  3,
                 thin = 5,
                 backend = 'cmdstan',
                 control =  list(adapt_delta =  0.99),
                 refresh = 0)
```

```{r fitModel2b, results='markdown', eval=TRUE, mhidden=TRUE}
data_gam.brm2 |> conditional_effects() |>  plot(points=TRUE)
data_gam.brm2 |> conditional_smooths() |>  plot()
```

#### Sample prior and posterior

```{r fitModel2c, results='markdown', eval=TRUE, mhidden=TRUE, cache=TRUE}
data_gam.brm3 <- update(data_gam.brm2, sample_prior = "yes", cores =  3, refresh =  0)
```

```{r fitModel2d, results='markdown', eval=TRUE, mhidden=TRUE}
data_gam.brm3 |> conditional_effects() |>  plot(points=TRUE)
data_gam.brm3 |> conditional_effects(spaghetti = TRUE, ndraws =  200) |>  plot(points=TRUE)
```

### Plotting prior and posterior

```{r fitModel2e, results='markdown', eval=TRUE, mhidden=TRUE, fig.width = 8, fig.height = 4, error = TRUE}
data_gam.brm3 |> get_variables()
data_gam.brm3 |> hypothesis('bs_sx_1 = 0', class = '') |> plot()
data_gam.brm3 |> hypothesis('sds_sx_1 = 0', class = '') |> plot()
data_gam.brm3 |> hypothesis('sigma = 0', class = '') |> plot()
data_gam.brm3 |> SUYR_prior_and_posterior()
```
::::
:::
<!-- END_PRIVATE-->

# MCMC sampling diagnostics 
<!-- START_PRIVATE-->
::: {.panel-tabset}
## brms
:::: {.panel-tabset}
### stan plots

```{r modelValidation2a, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=6, fig.height=4}
data_gam.brm3$fit |> stan_trace()
```

```{r modelValidation2b, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=6, fig.height=4}
data_gam.brm3$fit |> stan_ac() 
```

```{r modelValidation2c, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=6, fig.height=4}
data_gam.brm3$fit |> stan_rhat() 
```
```{r modelValidation2d, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=6, fig.height=4}
data_gam.brm3$fit |> stan_ess()
```

::::
:::
<!-- END_PRIVATE-->

# Model validation 
<!-- START_PRIVATE-->
::: {.panel-tabset}

## brms 
:::: {.panel-tabset}

### pp check

```{r modelValidation3a, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=6, fig.height=4}
data_gam.brm3 |> pp_check( type='dens_overlay', ndraws=100)
```

### DHARMa residuals

```{r modelValidation3b, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=8, fig.height=10}

data_gam.resids <- make_brms_dharma_res(data_gam.brm3, integerResponse = FALSE)
wrap_elements(~testUniformity(data_gam.resids)) +
               wrap_elements(~plotResiduals(data_gam.resids, form = factor(rep(1, nrow(data_gam))))) +
               wrap_elements(~plotResiduals(data_gam.resids, quantreg = FALSE)) +
               wrap_elements(~testDispersion(data_gam.resids))
testDispersion(data_gam.resids)
```
::::
:::
<!-- END_PRIVATE-->

# Partial effects plots 

<!-- START_PRIVATE-->
::: {.panel-tabset}

## brms 
:::: {.panel-tabset}

### conditional_effects

```{r partialPlot1a, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=8, fig.height=5}
data_gam.brm3 |> conditional_effects() |> plot(points = TRUE)
data_gam.brm3 |>
    conditional_effects(spaghetti = TRUE, ndraws = 250) |>
    plot(points = TRUE) 
```
::::
:::
<!-- END_PRIVATE-->

# Model investigation 

<!-- START_PRIVATE-->
::: {.panel-tabset}

## brms 
:::: {.panel-tabset}
### Summary

```{r summariseModel1a, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=8, fig.height=5}
data_gam.brm3 |> summary()
```

 - sds(sx_1) is the sd of the smooth weights (spline coefficients).
   This determines the amount of ‘wiggliness’, in an analogous way to
   how the sd of group-level effects in a varying slopes and
   intercepts model determine the amount of variability among groups
   in slopes and intercepts. 
   However, the actual numeric value of the
   sds() is not very practically interpretable, because thinking about
   the variance of smooth weights for any given data and model seems
   abstract to me. However, if the value is around zero, then this is
   like ‘complete-pooling’ of the basis functions, which means that
   there isn’t much added value of more than a single basis function.

- sx_1 is the unpenalized weight (ie coefficient) for one of the
  “natural” parameterized basis functions. The rest of the basis
  functions are like varying effects. Again, because the actual
  numeric value of sxs_1 is the value for the unpenalized coefficient
  for one of the basis functions, this wouldn’t seem to have a lot of
  practically interpretable meaning just from viewing this number.

### summarise_draws

```{r summariseModel1b, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=8, fig.height=5}
data_gam.brm3 |> get_variables()
data_gam.brm3 |>
  as_draws_df() |>
  dplyr::select(matches("^b_.*|^bs.*|^sds.*|^sigma$|^s_s.*")) |>
  summarise_draws(median,
    HDInterval::hdi,
    Pl = ~mean(.x < 0),
    Pg = ~mean(.x > 0)
    )
```
::::
:::
<!-- END_PRIVATE-->

# Further analyses 

<!-- START_PRIVATE-->
::: {.panel-tabset}

## How much has y increased up to an x value of 9

```{r aaa, results='markdown', eval=TRUE}
newdata <- with(data_gam, data.frame(x = c(min(x), 9)))
add_epred_draws(object = data_gam.brm3, newdata = newdata,
  ndraws =  2400) |>
  ungroup() |>
  group_by(.draw) |>
  summarise(Diff =  diff(.epred)) |>
  summarise(median_hdci(Diff),
    Pl = mean(Diff < 0),
    Pg =  mean(Diff > 0))

```


## find the optimum
:::: {.panel-tabset}
### derivatives
```{r Derivatives1a, results='markdown', eval=TRUE, mhidden=TRUE}
newdata <- with(data_gam, data.frame(x = seq(min(x), max(x), length = 1000)))
data_gam.peak <-
  add_epred_draws(object = data_gam.brm3, newdata = newdata, ndraws = 1000) |>
  ungroup() |> 
  group_by(.draw) |> 
   #summarise(x = x[which.max(.epred)]) |> 
  mutate(diff = .epred - lag(.epred)) |>
  summarise(x = x[which.min(abs(diff))]) |>
  median_hdci(x, .width = 0.95)
data_gam.peak

## lets plot this
data_gam.preds <-
  data_gam.brm3 |>
  add_epred_draws(newdata = newdata, object = _) |>
  ungroup() |>
  dplyr::select(-.row, -.chain, -.iteration) |> 
  group_by(x) |> 
  summarise_draws(median, HDInterval::hdi) |>
  ungroup() |> 
  mutate(Flag = between(x, data_gam.peak$.lower, data_gam.peak$.upper),
    Grp = data.table::rleid(Flag)
    )
data_gam.preds |> head()

ggplot(data_gam.preds, aes(y = median, x = x)) +
  geom_line(aes(colour = Flag, group = Grp))+
  geom_ribbon(aes(ymin = lower, ymax = upper, fill = Flag, group = Grp), alpha = 0.2)
```

### easystats
Unfortunately, it does not appear that this option provides confidence intervals.
```{r Derivatives1b, results='markdown', eval=TRUE, mhidden=TRUE}
data_gam.brm3 |>
  estimate_relation(keep_iterations = TRUE, length = 1000) |>
  estimate_smooth(x = "x")
```
::::

## find the largest positive slope
```{r Derivatives1c, results='markdown', eval=TRUE, mhidden=TRUE}
newdata <- with(data_gam, data.frame(x = seq(min(x), max(x), length = 1000)))
data_gam.brm3 |>
  add_epred_draws(newdata = newdata, object = _) |>
  ungroup() |> 
  group_by(.draw) |> 
  mutate(diff = .epred - lag(.epred)) |>
  summarise(
    maxGrad = max(abs(diff), na.rm = TRUE),
    x = x[which.max(diff)]) |>
  summarise_draws(median, HDInterval::hdi)

```
## find the greatest change in curvature
```{r Derivatives1d, results='markdown', eval=TRUE, mhidden=TRUE}
newdata <- with(data_gam, data.frame(x = seq(min(x), max(x), length = 1000)))
data_gam.brm3 |>
  add_epred_draws(newdata = newdata, object = _) |>
  filter(x > 3, x <13) |> 
  ungroup() |> 
  group_by(.draw) |> 
  mutate(diff = .epred - lag(.epred),
    diff2 = diff - lag(diff)) |>
  summarise(
    maxChange = max(abs(diff2), na.rm = TRUE),
    x = x[which.max(diff)]) |>
  summarise_draws(median, HDInterval::hdi)
```
:::
<!-- END_PRIVATE-->
