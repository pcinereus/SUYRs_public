---
title: "Bayesian GLM Part5"
author: "Murray Logan"
date: today
date-format: "DD/MM/YYYY"
format: 
  html:
    ## Format
    theme: [default, ../resources/ws-style.scss]
    css: ../resources/ws_style.css
    html-math-method: mathjax
    ## Table of contents
    toc: true
    toc-float: true
    ## Numbering
    number-sections: true
    number-depth: 3
    ## Layout
    page-layout: full
    fig-caption-location: "bottom"
    fig-align: "center"
    fig-width: 4
    fig-height: 4
    fig-dpi: 72
    tbl-cap-location: top
    ## Code
    code-fold: false
    code-tools: true
    code-summary: "Show the code"
    code-line-numbers: true
    code-block-border-left: "#ccc"
    code-copy: true
    highlight-style: atom-one
    ## Execution
    execute:
      echo: true
      cache: true
    ## Rendering
    embed-resources: true
crossref:
  fig-title: '**Figure**'
  fig-labels: arabic
  tbl-title: '**Table**'
  tbl-labels: arabic
engine: knitr
output_dir: "docs"
documentclass: article
fontsize: 12pt
mainfont: Arial
mathfont: LiberationMono
monofont: DejaVu Sans Mono
classoption: a4paper
bibliography: ../resources/references.bib
---

```{r}
#| label: setup
#| include: false

knitr::opts_chunk$set(cache.lazy = FALSE,
                      tidy = "styler")
options(tinytex.engine = "xelatex")
```

# Preparations

Load the necessary libraries

```{r}
#| label: libraries
#| output: false
#| eval: true
#| warning: false
#| message: false
#| cache: false

library(tidyverse)     #for data wrangling etc
library(rstanarm)      #for fitting models in STAN
library(cmdstanr)      #for cmdstan
library(brms)          #for fitting models in STAN
library(coda)          #for diagnostics
library(bayesplot)     #for diagnostics
library(ggmcmc)        #for MCMC diagnostics
library(DHARMa)        #for residual diagnostics
library(rstan)         #for interfacing with STAN
library(emmeans)       #for marginal means etc
library(broom)         #for tidying outputs
library(tidybayes)     #for more tidying outputs
library(ggeffects)     #for partial plots
library(broom.mixed)   #for summarising models
library(ggeffects)     #for partial effects plots
library(bayestestR)    #for ROPE
library(see)           #for some plots
library(easystats)     #for the easystats ecosystem
library(ggridges)      #for ridge plots
library(patchwork)     #for multiple plots
library(modelsummary)  #for data and model summaries 
theme_set(theme_grey()) #put the default ggplot theme back
source("helperFunctions.R")
```

# Scenario

Here is a modified example from @Quinn-2002-2002. Day and Quinn
(1989) described an experiment that examined how rock surface type
affected the recruitment of barnacles to a rocky shore. The experiment
had a single factor, surface type, with 4 treatments or levels: algal
species 1 (ALG1), algal species 2 (ALG2), naturally bare surfaces (NB)
and artificially scraped bare surfaces (S). There were 5 replicate plots
for each surface type and the response (dependent) variable was the
number of newly recruited barnacles on each plot after 4 weeks.

![Six-plated barnacle](../resources/barnacles.jpg){#fig-barnacle width="224" height="308"}

:::: {.columns}

::: {.column width="50%"}

TREAT   BARNACLE
------- ----------
ALG1    27
..      ..
ALG2    24
..      ..
NB      9
..      ..
S       12
..      ..

: Format of day.csv data files {#tbl-day .table-condensed}

:::

::: {.column width="50%"}

-------------- ----------------------------------------------------------------------------------------------------------------------------------------------
**TREAT**      Categorical listing of surface types. ALG1 = algal species 1, ALG2 = algal species 2, NB = naturally bare surface, S = scraped bare surface.
**BARNACLE**   The number of newly recruited barnacles on each plot after 4 weeks.
-------------- ----------------------------------------------------------------------------------------------------------------------------------------------

: Description of the variables in the day data file {#tbl-day1 .table-condensed}

:::
::::

# Read in the data

```{r}
#| label: readData

day <- read_csv('../data/day.csv', trim_ws = TRUE)
```

<!-- START_PRIVATE-->
::: {.panel-tabset}

## glimpse
```{r}
#| label: examinData
glimpse(day)
```

## head
```{r}
## Explore the first 6 rows of the data
head(day)
```

## str
```{r}
str(day)
```

## Easystats (datawizard)
```{r}
day |> datawizard::data_codebook()
```

## Skim (modelsummary)
```{r}
day |> modelsummary::datasummary_skim()
day |> modelsummary::datasummary_skim(by = "TREAT")
```


:::
<!-- END_PRIVATE-->


Start by declaring the categorical variables as factor.

```{r prepare, results='markdown', eval=TRUE, mhidden=TRUE}
day <- day |> mutate(TREAT = factor(TREAT))
```

Model formula:
$$
\begin{align}
y_i &\sim{} \mathcal{Pois}(\lambda_i)\\
ln(\mu_i) &= \boldsymbol{\beta} \bf{X_i}\\
\beta_0 &\sim{} \mathcal{N}(3.1, 1)\\
\beta_{1,2,3} &\sim{} \mathcal{N}(0,1)\\
\end{align}
$$

where $\boldsymbol{\beta}$ is a vector of effects parameters and $\bf{X}$ is a model matrix representing the intercept and treatment contrasts for the effects of Treatment on barnacle recruitment.

# Exploratory data analysis 
<!-- START_PRIVATE-->

The exploratory data analyses that we performed in the frequentist instalment
of this example are equally valid here.  That is, boxplots and/or violin plots for each population
(substrate type).

```{r EDA, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=6, fig.height=6, mhidden=TRUE}
day |> ggplot(aes(y = BARNACLE, x = TREAT)) +
    geom_boxplot()+
    geom_point(color = 'red')
day |> ggplot(aes(y = BARNACLE, x = TREAT)) +
    geom_violin()+
    geom_point(color = 'red')
```

**Conclusions:**

- although exploratory data analysis suggests that we might well be fine
modelling these data against a Gaussian distribution, a Poisson distribution
would clearly be a more natural choice and it would also prevent any non
positive predictions.

<!-- END_PRIVATE-->

# Fit the model

<!-- START_PRIVATE-->
::: {.panel-tabset}

## rstanarm 
:::: {.panel-tabset}

### Using default priors
In `rstanarm`, the default priors are designed to be weakly informative. They
are chosen to provide moderate regularisation (to help prevent over-fitting) and
help stabilise the computations.

```{r fitModel1a, results='markdown', eval=TRUE, mhidden=TRUE}
day.rstanarm = stan_glm(BARNACLE ~ TREAT, data=day,
                        family=poisson(link='log'), 
                         iter = 5000, warmup = 1000,
                         chains = 3, thin = 5, refresh = 0)
```

```{r fitModel1b, results='markdown', eval=TRUE, mhidden=TRUE}
prior_summary(day.rstanarm)
```

This tells us:

- for the intercept, when the family is Poisson, it is using a normal prior with a mean of 0 and a
  standard deviation of 2.5.  The 2.5 is used for all intercepts.  It is often
  scaled, but only if it is larger than 2.5 is the scaled version kept.
  
- for the coefficients (in this case, just the slope), the default prior is a
normal prior centred around 0 with a standard deviation of 2.5.  This is then
adjusted for the scale of the data by dividing the 2.5 by the
standard deviation of the numerical dummy
variables for the predictor (then rounded). 

```{r fitModel1d, results='markdown', eval=TRUE, mhidden=TRUE}
2.5/sd(model.matrix(~TREAT, day)[, 2])
```

-  there is no auxiliary prior as we are employing a Poisson distribution.
   
### Assessing priors

One way to assess the priors is to have the MCMC sampler sample purely from the
prior predictive distribution without conditioning on the observed data.  Doing
so provides a glimpse at the range of predictions possible under the priors.  On
the one hand, wide ranging predictions would ensure that the priors are unlikely
to influence the actual predictions once they are conditioned on the data.  On
the other hand, if they are too wide, the sampler is being permitted to traverse
into regions of parameter space that are not logically possible in the context
of the actual underlying ecological context.  Not only could this mean that
illogical parameter estimates are possible, when the sampler is traversing
regions of parameter space that are not supported by the actual data, the sampler
can become unstable and have difficulty.

We can draw from the prior predictive distribution instead of conditioning on
the response, by updating the model and indicating `prior_PD=TRUE`.  After
refitting the model in this way, we can plot the predictions to gain insights
into the range of predictions supported by the priors alone.

```{r fitModel1f, results='markdown', eval=TRUE, mhidden=TRUE}
day.rstanarm1 <- update(day.rstanarm,  prior_PD=TRUE)
```

```{r fitModel1g, results='markdown', eval=TRUE, mhidden=TRUE}
day.rstanarm1 |>
    ggpredict() |>
    plot(show_data = TRUE)

ggpredict(day.rstanarm1) |>
    plot(show_data=TRUE) |>
    wrap_plots() &
    scale_y_log10()
```

**Conclusions:**

- we see that the range of predictions is fairly wide and the predicted means could range
  from 0 to very large (perhaps too large).


### Defining priors

The following link provides some guidance about defining priors.
[https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations]

When defining our own priors, we typically do not want them to be scaled.

If we wanted to define our own priors that were less vague, yet still not likely
to bias the outcomes, we could try the following priors (mainly plucked out of
thin air):

- $\beta_0$: normal centred at 3 with a standard deviation of 10
  - mean of 3: since `mean(log(day$BARNACLE))`
  - sd of 1: since `sd(log(day$BARNACLE))`
- $\beta_1$: normal centred at 0 with a standard deviation of 6
  - sd of 1: since: `sd(log(day$BARNACLE))/apply(model.matrix(~TREAT, data = day), 2, sd)`
```{r, echo = FALSE, eval = FALSE}
day |>
    group_by(TREAT) |>
    summarise(mean(log(BARNACLE)),
              sd(log(BARNACLE)))

model.matrix(~TREAT, data = day)
apply(model.matrix(~TREAT, data = day), 2, sd)
sd(log(day$BARNACLE))/apply(model.matrix(~TREAT, data = day), 2, sd)

## day |>
##     group_by(TREAT) |>
##     summarise(BARNACLE = sample(log(BARNACLE), 1000, replace = TRUE),
##               R = 1:1000) |>
##     ungroup() |>
##     group_by(R) |>
##     summarise(D = diff(BARNACLE), Comp = 1:3) |>
##     ungroup() |>
##     group_by(Comp) |>
##     summarise(sd(D))
```

I will also overlay the raw data for comparison.

```{r fitModel1h, results='markdown', eval=TRUE, mhidden=TRUE}
day.rstanarm2 <- stan_glm(BARNACLE ~ TREAT, data=day,
                        family=poisson(link='log'), 
                         prior_intercept = normal(3, 1, autoscale=FALSE),
                         prior = normal(0, 1, autoscale=FALSE),
                         prior_PD=TRUE, 
                         iter = 5000, warmup = 1000,
                         chains = 3, thin = 5, refresh = 0
                         )
```

```{r fitModel1i, results='markdown', eval=TRUE, mhidden=TRUE}
ggpredict(day.rstanarm2) |>
  plot(show_data=TRUE) +
  scale_y_log10()
```

Now lets refit, conditioning on the data.

```{r fitModel1j, results='markdown', eval=TRUE, mhidden=TRUE}
day.rstanarm3= update(day.rstanarm2,  prior_PD=FALSE) 
```

### Plotting prior and posterior

```{r modelFit1k, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=6, fig.height=4}
posterior_vs_prior(day.rstanarm3, color_by='vs', group_by=TRUE,
                   facet_args=list(scales='free_y'))
```

**Conclusions:**

- in each case, the prior is substantially wider than the posterior, suggesting
  that the posterior is not biased towards the prior.
  
```{r modelFit1l, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=6, fig.height=4}
ggpredict(day.rstanarm3) |> plot(show_data=TRUE)
```
::::
## brms 

:::: {.panel-tabset}
### Using default priors

In `brms`, the default priors are designed to be weakly informative.  They are
chosen to provide moderate regularisation (to help prevent over-fitting) and
help stabilise the computations.

Unlike `rstanarm`, `brms` models must be compiled before they start sampling.
For most models, the compilation of the stan code takes around 45 seconds.

```{r fitModel2a, results='markdown', eval=TRUE, mhidden=TRUE}
day.form <- bf(BARNACLE ~ TREAT,
               family = poisson(link = 'log'))
day.brm <- brm(day.form, 
               data = day,
               iter = 5000,
               warmup = 2500,
               chains = 3, cores = 3,
               thin = 5,
               refresh = 0,
               backend = "cmdstan")
```

```{r fitModel2b, results='markdown', eval=TRUE, mhidden=TRUE, paged.print=FALSE,tidy.opts = list(width.cutoff = 80), echo=2}
options(width=100)
day.brm |> prior_summary()
options(width=80)
```

This tells us:

- for the intercept, it is using a student t (flatter normal) prior with a mean of 0 and a
  standard deviation of 2.5.  These mean and standard deviation values are
  the defaults.
  
- for the beta coefficients (in this case, each effect), the default prior is
  a improper flat prior. A flat prior essentially means that any value between
  negative infinity and positive infinity are equally likely. Whilst this might
  seem reckless, in practice, it seems to work reasonably well for non-intercept
  beta parameters.

- since we have nominated a Poisson distribution, there is no auxiliary prior.
  
### Assessing priors
::::: {.panel-tabset}

One way to assess the priors is to have the MCMC sampler sample purely from the
prior predictive distribution without conditioning on the observed data.  Doing
so provides a glimpse at the range of predictions possible under the priors.  On
the one hand, wide ranging predictions would ensure that the priors are unlikely
to influence the actual predictions once they are conditioned on the data.  On
the other hand, if they are too wide, the sampler is being permitted to traverse
into regions of parameter space that are not logically possible in the context
of the actual underlying ecological context.  Not only could this mean that
illogical parameter estimates are possible, when the sampler is traversing
regions of parameter space that are not supported by the actual data, the sampler
can become unstable and have difficulty.

In `brms`, we can inform the sampler to draw from the prior predictive
distribution instead of conditioning on the response, by running the model with
the `sample_prior='only'` argument.  Unfortunately, this cannot be applied when
there are flat priors (since the posteriors will necessarily extend to negative
and positive infinity).  Therefore, in order to use this useful routine, we need
to make sure that we have defined a proper prior for all parameters.


```{r fitModel2d, results='markdown', eval=TRUE, mhidden=TRUE}
model.matrix(~TREAT, data = day)
apply(model.matrix(~TREAT, data = day), 2, sd)
sd(log(day$BARNACLE))/apply(model.matrix(~TREAT, data = day), 2, sd)

priors <- prior(normal(0, 1),  class = 'b')
day.brm1 <- brm(day.form,
               data = day,
               prior = priors, 
               sample_prior = 'only', 
               iter = 5000,
               warmup = 2500,
               chains = 3, cores = 3,
               thin = 5,
               refresh = 0,
               backend = "cmdstan")
```


#### ggpredict

```{r fitModel2e, results='markdown', eval=TRUE, mhidden=TRUE}
day.brm1 |> ggpredict() |> plot(show_data=TRUE)
day.brm1 |>
    ggpredict() |>
    plot(show_data=TRUE) |>
    wrap_plots() &
    scale_y_log10()
## day.brm1 |> ggpredict() |> plot(show_data=TRUE) |> `[[`(1) + scale_y_log10()
```

#### ggemmeans

```{r fitModel2e2, results='markdown', eval=TRUE, mhidden=TRUE}
day.brm1 |>
    ggemmeans(~TREAT) |>
    plot(show_data=TRUE) +
    scale_y_log10()
```

#### conditional_effects


```{r fitModel2e3, results='markdown', eval=TRUE, mhidden=TRUE}
day.brm1 |> 
    conditional_effects() |>
    plot(points=TRUE)
day.brm1 |>
    conditional_effects() |>
    plot(points=TRUE) |>
    wrap_plots() &
    scale_y_log10()
```
:::::

**Conclusions:**

- we see that the range of predictions is fairly wide (ranging from 0 to very high)

### Defining priors
::::: {.panel-tabset}

The following link provides some guidance about defining priors.
[https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations]

When defining our own priors, we typically do not want them to be scaled.

If we wanted to define our own priors that were less vague, yet still not likely
to bias the outcomes, we could try the following priors (mainly plucked out of
thin air):

- $\beta_0$: normal centred at 3 with a standard deviation of 10
  - mean of 3: since `mean(log(day$BARNACLE))`
  - sd of 1: since `sd(log(day$BARNACLE))`
- $\beta_1$: normal centred at 0 with a standard deviation of 6
  - sd of 1: since: `sd(log(day$BARNACLE))/apply(model.matrix(~TREAT, data = day), 2, sd)`
```{r, echo = FALSE, eval = FALSE}
day |>
    group_by(TREAT) |>
    summarise(mean(log(BARNACLE)),
              sd(log(BARNACLE)))

model.matrix(~TREAT, data = day)
apply(model.matrix(~TREAT, data = day), 2, sd)
sd(log(day$BARNACLE))/apply(model.matrix(~TREAT, data = day), 2, sd)

## day |>
##     group_by(TREAT) |>
##     summarise(BARNACLE = sample(log(BARNACLE), 1000, replace = TRUE),
##               R = 1:1000) |>
##     ungroup() |>
##     group_by(R) |>
##     summarise(D = diff(BARNACLE), Comp = 1:3) |>
##     ungroup() |>
##     group_by(Comp) |>
##     summarise(sd(D))
```

I will also overlay the raw data for comparison.

```{r fitModel2h, results='markdown', eval=TRUE, mhidden=TRUE}
priors <- prior(normal(3.1, 2.2),  class = 'Intercept') +
    prior(normal(0, 2.4), class = 'b')
day.form <- bf(BARNACLE ~ TREAT,
                   family = poisson(link = 'log'))
day.brm2 <- brm(day.form, 
                data = day,
                prior = priors, 
                sample_prior = 'only', 
                iter = 5000,
                warmup = 1000,
                chains = 3, cores = 3,
                thin = 5,
                refresh = 0,
                seed = 1,
                backend = "cmdstan")
```

#### ggpredict

```{r fitModel2i, results='markdown', eval=TRUE, mhidden=TRUE}
day.brm2 |> ggpredict() |> plot(show_data=TRUE)
day.brm2 |>
    ggpredict() |>
    plot(show_data=TRUE) |>
    wrap_plots() &
    scale_y_log10()
```

#### ggemmeans

```{r fitModel2i2, results='markdown', eval=TRUE, mhidden=TRUE}
day.brm2 |> ggemmeans(~TREAT) |> plot(show_data=TRUE)
day.brm2 |>
    ggemmeans(~TREAT) |>
    plot(show_data=TRUE) |>
    wrap_plots() &
    scale_y_log10()
```

#### conditional_effects

```{r fitModel2i3, results='markdown', eval=TRUE, mhidden=TRUE}
day.brm2 |> conditional_effects() |>  plot(points=TRUE)
day.brm2 |>
    conditional_effects() |>
    plot(points=TRUE) |>
    wrap_plots() &
    scale_y_log10()
```

:::::

```{r fitModel2j, results='markdown', eval=TRUE, mhidden=TRUE}
day.brm3 <- day.brm2 |> update(sample_prior = 'yes', refresh = 0) 
```

### Plotting prior and posterior

```{r fitModel2k, results='markdown', eval=TRUE, mhidden=TRUE}
day.brm3 |> get_variables()
day.brm3 |> hypothesis('TREATALG2<0') |> plot()
day.brm3 |> hypothesis('TREATNB<0') |> plot()
day.brm3 |> hypothesis('TREATS<0') |> plot()
day.brm3 |> SUYR_prior_and_posterior()
```

### Exploring the stan code

```{r fitModel2l, results='markdown', eval=TRUE, mhidden=TRUE}
day.brm3 |> standata()
day.brm3 |> stancode()
```

::::
:::
<!-- END_PRIVATE-->

# MCMC sampling diagnostics
<!-- START_PRIVATE-->
::: {.panel-tabset}

In addition to the regular model diagnostics checking, for Bayesian analyses, it
is also necessary to explore the MCMC sampling diagnostics to be sure that the
chains are well mixed and have converged on a stable posterior.

There are a wide variety of tests that range from the big picture, overall chain
characteristics to the very specific detailed tests that allow the experienced
modeller to drill down to the very fine details of the chain behaviour.
Furthermore, there are a multitude of packages and approaches for exploring
these diagnostics.

## rstanarm 
:::: {.panel-tabset}

### bayesplot

The `bayesplot` package offers a range of MCMC diagnostics as well as Posterior
Probability Checks (PPC), all of which have a convenient `plot()` interface.
Lets start with the MCMC diagnostics.

<details><summary>See list of available diagnostics by name</summary>
```{r modelValidation1a, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=6, fig.height=4}
available_mcmc()
```
</details>

Of these, we will focus on:

- mcmc_trace: this plots the estimates of each parameter over the post-warmup
  length of each MCMC chain.  Each chain is plotted in a different shade of
  blue, with each parameter in its own facet.  Ideally, each **trace** should
  just look like noise without any discernible drift and each of the traces for
  a specific parameter should look the same (i.e, should not be displaced above
  or below any other trace for that parameter).

```{r modelValidation1b, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=6, fig.height=4}
plot(day.rstanarm3, plotfun='mcmc_trace')
```
  
   The chains appear well mixed and very similar
   
- acf (auto-correlation function): plots the auto-correlation between successive
  MCMC sample lags for each parameter and each chain
  
```{r modelValidation1c, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=6, fig.height=4}
plot(day.rstanarm3, 'acf_bar')
```

   There is no evidence of auto-correlation in the MCMC samples

- Rhat: Rhat is a measure of convergence between the chains.  The closer the
  values are to 1, the more the chains have converged.  Values greater than 1.05
  indicate a lack of convergence.  There will be an Rhat value for each
  parameter estimated.

```{r modelValidation1d, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=6, fig.height=4}
plot(day.rstanarm3, 'rhat_hist')
```

  All Rhat values are below 1.05, suggesting the chains have converged.
  
- neff (number of effective samples): the ratio of the number of effective
  samples (those not rejected by the sampler) to the number of samples provides
  an indication of the effectiveness (and efficiency) of the MCMC sampler.
  Ratios that are less than 0.5 for a parameter suggest that the sampler spent
  considerable time in difficult areas of the sampling domain and rejected more
  than half of the samples (replacing them with the previous effective sample).  
  
  If the ratios are low, tightening the priors may help.
  
```{r modelValidation1e, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=6, fig.height=4}
plot(day.rstanarm3, 'neff_hist')
```

  Ratios all very high.

<details><summary>More diagnostics</summary>
```{r Validation1f, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=6, fig.height=4}
plot(day.rstanarm3, 'combo')
plot(day.rstanarm3, 'violin')
```
</details>


### stan plots

The `rstan` package offers a range of MCMC diagnostics.
Lets start with the MCMC diagnostics.

Of these, we will focus on:

- stan_trace: this plots the estimates of each parameter over the post-warmup
  length of each MCMC chain. Each chain is plotted in a different colour, with
  each parameter in its own facet. Ideally, each **trace** should just look like
  noise without any discernible drift and each of the traces for a specific
  parameter should look the same (i.e, should not be displaced above or below
  any other trace for that parameter).
  
```{r modelValidation1g, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=6, fig.height=4}
stan_trace(day.rstanarm3)
```

   The chains appear well mixed and very similar
   
- stan_acf (autocorrelation function): plots the autocorrelation between successive
  MCMC sample lags for each parameter and each chain
  
```{r modelValidation1h, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=6, fig.height=4}
stan_ac(day.rstanarm3) 
```

   There is no evidence of auto-correlation in the MCMC samples

- stan_rhat: Rhat is a **scale reduction factor** measure of convergence between the chains.  The closer the
  values are to 1, the more the chains have converged.  Values greater than 1.05
  indicate a lack of convergence.  There will be an Rhat value for each
  parameter estimated.

```{r modelValidation1i, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=6, fig.height=4}
stan_rhat(day.rstanarm3) 
```

  All Rhat values are below 1.05, suggesting the chains have converged.
  
- stan_ess (number of effective samples): the ratio of the number of effective
  samples (those not rejected by the sampler) to the number of samples provides
  an indication of the effectiveness (and efficiency) of the MCMC sampler.
  Ratios that are less than 0.5 for a parameter suggest that the sampler spent
  considerable time in difficult areas of the sampling domain and rejected more
  than half of the samples (replacing them with the previous effective sample).  
  
  If the ratios are low, tightening the priors may help.
  
```{r modelValidation1j, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=6, fig.height=4}
stan_ess(day.rstanarm3)
```

  Ratios all very high.

```{r modelValidation1k, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=6, fig.height=4}
stan_dens(day.rstanarm3, separate_chains = TRUE)
```


### ggmcmc

The `ggmean` package also has a set of MCMC diagnostic functions.
Lets start with the MCMC diagnostics.

Of these, we will focus on:

- ggs_traceplot: this plots the estimates of each parameter over the post-warmup
  length of each MCMC chain. Each chain is plotted in a different colour, with
  each parameter in its own facet. Ideally, each **trace** should just look like
  noise without any discernible drift and each of the traces for a specific
  parameter should look the same (i.e, should not be displaced above or below
  any other trace for that parameter).

```{r modelValidation1l, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=6, fig.height=12}
day.ggs <- ggs(day.rstanarm3)
ggs_traceplot(day.ggs)
```

   The chains appear well mixed and very similar
   
- gss_autocorrelation (autocorrelation function): plots the autocorrelation between successive
  MCMC sample lags for each parameter and each chain
  
```{r modelValidation1m, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=6, fig.height=12}
ggs_autocorrelation(day.ggs)
```

   There is no evidence of auto-correlation in the MCMC samples

- stan_rhat: Rhat is a **scale reduction factor** measure of convergence between the chains.  The closer the
  values are to 1, the more the chains have converged.  Values greater than 1.05
  indicate a lack of convergence.  There will be an Rhat value for each
  parameter estimated.

```{r modelValidation1n, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=6, fig.height=4}
ggs_Rhat(day.ggs)
```

  All Rhat values are below 1.05, suggesting the chains have converged.
  
- stan_ess (number of effective samples): the ratio of the number of effective
  samples (those not rejected by the sampler) to the number of samples provides
  an indication of the effectiveness (and efficiency) of the MCMC sampler.
  Ratios that are less than 0.5 for a parameter suggest that the sampler spent
  considerable time in difficult areas of the sampling domain and rejected more
  than half of the samples (replacing them with the previous effective sample).  
  
  If the ratios are low, tightening the priors may help.
  
```{r modelValidation1o, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=6, fig.height=4}
ggs_effective(day.ggs)
```

  Ratios all very high.

<details><summary>More diagnostics</summary>
```{r modelValidation1p, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=6, fig.height=4}
ggs_crosscorrelation(day.ggs)
```

```{r modelValidation1q, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=6, fig.height=4}
ggs_grb(day.ggs)
```
</details>

::::
## brms 

### stan plots

The `brms` package offers a range of MCMC diagnostics.
Lets start with the MCMC diagnostics.

Of these, we will focus on:

- stan_trace: this plots the estimates of each parameter over the post-warmup
  length of each MCMC chain. Each chain is plotted in a different colour, with
  each parameter in its own facet. Ideally, each **trace** should just look like
  noise without any discernible drift and each of the traces for a specific
  parameter should look the same (i.e, should not be displaced above or below
  any other trace for that parameter).
  
```{r modelValidation2g, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=6, fig.height=4}
day.brm3$fit |> stan_trace()
day.brm3$fit |> stan_trace(inc_warmup=TRUE)
```

   The chains appear well mixed and very similar
   
- stan_acf (auto-correlation function): plots the auto-correlation between successive
  MCMC sample lags for each parameter and each chain
  
```{r modelValidation2h, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=6, fig.height=4}
day.brm3$fit |> stan_ac() 
```

   There is no evidence of auto-correlation in the MCMC samples

- stan_rhat: Rhat is a **scale reduction factor** measure of convergence between the chains.  The closer the
  values are to 1, the more the chains have converged.  Values greater than 1.05
  indicate a lack of convergence.  There will be an Rhat value for each
  parameter estimated.

```{r modelValidation2i, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=6, fig.height=4}
day.brm3$fit |> stan_rhat() 
```

  All Rhat values are below 1.05, suggesting the chains have converged.
  
- stan_ess (number of effective samples): the ratio of the number of effective
  samples (those not rejected by the sampler) to the number of samples provides
  an indication of the effectiveness (and efficiency) of the MCMC sampler.
  Ratios that are less than 0.5 for a parameter suggest that the sampler spent
  considerable time in difficult areas of the sampling domain and rejected more
  than half of the samples (replacing them with the previous effective sample).  
  
  If the ratios are low, tightening the priors may help.
  
```{r modelValidation2j, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=6, fig.height=4}
day.brm3$fit |> stan_ess()
```

  Ratios all very high.

```{r modelValidation2k, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=6, fig.height=4}
day.brm3$fit |> stan_dens(separate_chains = TRUE)
```
:::
<!-- END_PRIVATE-->


# Model validation 

<!-- START_PRIVATE-->
::: {.panel-tabset}

## rstanarm 
:::: {.panel-tabset}

### pp check

Post predictive checks provide additional diagnostics about the fit of the
model.  Specifically, they provide a comparison between predictions drawn from
the model and the observed data used to train the model.

<details><summary>See list of available diagnostics by name</summary>
```{r modelValidation3a, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=6, fig.height=4}
available_ppc()
```
</details>

- dens_overlay: plots the density distribution of the observed data (black line)
overlayed on top of 50 density distributions generated from draws from the model
(light blue).  Ideally, the 50 realisations should be roughly consistent with
the observed data.

```{r modelValidation3b, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=6, fig.height=4}
pp_check(day.rstanarm3,  plotfun='dens_overlay')
```

The model draws appear deviate from the observed data.

- error_scatter_avg: this plots the observed values against the average
  residuals. Similar to a residual plot, we do not want to see any patterns in
  this plot.  There is some pattern remaining in these residuals.

```{r modelValidation3c, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=6, fig.height=4}
pp_check(day.rstanarm3, plotfun='error_scatter_avg')
```

The predictive error seems to be related to the predictor - the model performs
poorest at higher mussel clump areas.


- error_scatter_avg_vs_x: this is similar to a regular residual plot and as such
  should be interpreted as such.  Again, this is not interpretable for binary data.

```{r modelValidation3d, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=6, fig.height=4}
pp_check(day.rstanarm3, x=as.numeric(day$TREAT), plotfun='error_scatter_avg_vs_x')
```

- intervals:  plots the observed data overlayed on top of posterior predictions
associated with each level of the predictor.  Ideally, the observed data should
all fall within the predictive intervals.


```{r modelValidation3e, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=6, fig.height=4}
pp_check(day.rstanarm3, x=as.numeric(day$TREAT), plotfun='intervals')
```

The modelled predictions seem to underestimate the uncertainty with increasing
mussel clump area.

The `shinystan` package allows the full suite of MCMC diagnostics and posterior
predictive checks to be accessed via a web interface.

```{r modelValidation3g, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=6, fig.height=4}
#library(shinystan)
#launch_shinystan(day.rstanarm3)
```

### DHARMa residuals

DHARMa residuals provide very useful diagnostics.  Unfortunately, we cannot
directly use the `simulateResiduals()` function to generate the simulated
residuals.  However, if we are willing to calculate some of the components
yourself, we can still obtain the simulated residuals from the fitted stan model.

We need to supply:

- simulated (predicted) responses associated with each observation.
- observed values
- fitted (predicted) responses (averaged) associated with each observation

```{r modelValidation4a, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=8, fig.height=5}
preds <- posterior_predict(day.rstanarm3,  ndraws=250,  summary=FALSE)
day.resids <- createDHARMa(simulatedResponse = t(preds),
                            observedResponse = day$BARNACLE,
                            fittedPredictedResponse = apply(preds, 2, median),
                            integerResponse = TRUE)
plot(day.resids)
```

**Conclusions:**

- the simulated residuals DOES NOT suggest any issues with the fitted model 

::::

## brms 
:::: {.panel-tabset}

### pp check
Post predictive checks provide additional diagnostics about the fit of the
model.  Specifically, they provide a comparison between predictions drawn from
the model and the observed data used to train the model.

<details><summary>See list of available diagnostics by name</summary>
```{r modelValidation5a, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=6, fig.height=4}
available_ppc()
```
</details>

- dens_overlay: plots the density distribution of the observed data (black line)
overlayed on top of 50 density distributions generated from draws from the model
(light blue).  Ideally, the 50 realisations should be roughly consistent with
the observed data.

```{r modelValidation5b, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=6, fig.height=4}
day.brm3 |> pp_check(type = 'dens_overlay', ndraws=200)
```

The model draws appear deviate from the observed data.

- error_scatter_avg: this plots the observed values against the average
  residuals. Similar to a residual plot, we do not want to see any patterns in
  this plot.  There is some pattern remaining in these residuals.

```{r modelValidation5c, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=6, fig.height=4}
day.brm3 |> pp_check(type = 'error_scatter_avg')
```

The predictive error seems to be related to the predictor - the model performs
poorest at higher mussel clump areas.

- intervals:  plots the observed data overlayed on top of posterior predictions
associated with each level of the predictor.  Ideally, the observed data should
all fall within the predictive intervals.


```{r modelValidation5e, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=6, fig.height=4}
day.brm3 |> pp_check(group='TREAT', type='intervals')
```

The modelled predictions seem to underestimate the uncertainty with increasing
mussel clump area.

The `shinystan` package allows the full suite of MCMC diagnostics and posterior
predictive checks to be accessed via a web interface.

```{r modelValidation5g, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=6, fig.height=4}
#library(shinystan)
#launch_shinystan(day.brm3)
```

### DHARMa residuals

DHARMa residuals provide very useful diagnostics.  Unfortunately, we cannot
directly use the `simulateResiduals()` function to generate the simulated
residuals.  However, if we are willing to calculate some of the components
yourself, we can still obtain the simulated residuals from the fitted stan model.

We need to supply:

- simulated (predicted) responses associated with each observation.
- observed values
- fitted (predicted) responses (averaged) associated with each observation

```{r modelValidation6a, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=8, fig.height=5}
preds <- day.brm3 |> posterior_predict(ndraws = 250,  summary = FALSE)
day.resids <- createDHARMa(simulatedResponse = t(preds),
                            observedResponse = day$BARNACLE,
                            fittedPredictedResponse = apply(preds, 2, median),
                            integerResponse = TRUE)
day.resids |> plot()
```
```{r modelValidation6aa, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=8, fig.height=10}
day.resids <- make_brms_dharma_res(day.brm3, integerResponse = TRUE)
wrap_elements(~testUniformity(day.resids)) +
               wrap_elements(~plotResiduals(day.resids, form = factor(rep(1, nrow(day))))) +
               wrap_elements(~plotResiduals(day.resids, quantreg = TRUE)) +
               wrap_elements(~testDispersion(day.resids))

```

**Conclusions:**

- the simulated residuals DO NOT suggest any issues with the model fit

::::
:::
<!-- END_PRIVATE-->

# Partial effects plots 

<!-- START_PRIVATE-->
::: {.panel-tabset}

## rstanarm 
:::: {.panel-tabset}

### ggpredict

```{r partialPlot1a, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=8, fig.height=5}
day.rstanarm3 |> ggpredict() |> plot(show_data=TRUE)
```

### ggemmeans

```{r partialPlot1b, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=8, fig.height=5}
day.rstanarm3 |> ggemmeans(~TREAT,  type='fixed') |> plot(show_data=TRUE)
```

### epred_draws

```{r partialPlot1c, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=8, fig.height=5}
day.rstanarm3 |>
    epred_draws(newdata=day) |>
  median_hdci() |>
  ggplot(aes(x=TREAT, y=.epred)) +
  geom_pointrange(aes(ymin=.lower, ymax=.upper)) + 
  geom_line() +
  geom_point(data=day,  aes(y=BARNACLE,  x=TREAT))
```
::::
## brms 
:::: {.panel-tabset}

### conditional_effects

```{r partialPlot2d, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=8, fig.height=5}
day.brm3 |> conditional_effects() |> plot(points = TRUE)
```

### ggpredict

```{r partialPlot2a, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=8, fig.height=5}
day.brm3 |> ggpredict() |> plot(show_data = TRUE)
```


### ggemmeans

```{r partialPlot2b, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=8, fig.height=5}
day.brm3 |> ggemmeans(~TREAT) |> plot(show_data = TRUE)
```

### epred_draws

```{r partialPlot2c, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=8, fig.height=5}
day.brm3 |>
    epred_draws(newdata = day) |>
    median_hdci() |>
    ggplot(aes(x = TREAT, y = .epred)) +
    geom_pointrange(aes(ymin = .lower, ymax = .upper)) + 
    geom_line() +
    geom_point(data = day,  aes(y = BARNACLE,  x = TREAT))
```

::::
:::
<!-- END_PRIVATE-->

# Model investigation 

<!-- START_PRIVATE-->
::: {.panel-tabset}

## rstanarm 
:::: {.panel-tabset}

`rstanarm` captures the MCMC samples from `stan` within the returned list.
There are numerous ways to retrieve and summarise these samples.  The first
three provide convenient numeric summaries from which you can draw conclusions,
the last four provide ways of obtaining the full posteriors. 

### summary

The `summary()` method generates simple summaries (mean, standard deviation as
well as 10, 50 and 90 percentiles).

```{r summariseModel1a, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=8, fig.height=5}
summary(day.rstanarm3)
```

```{r summariseModel1a1, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=8, fig.height=5, echo=FALSE}
day.sum <- summary(day.rstanarm3)
```

**Conclusions:**

- in the Model Info, we are informed that the total MCMC posterior sample size
  is `r nrow(as.matrix(day.rstanarm3))` and that there were 20 raw observations.
- the estimated mean (expected number of newly recruited barnacles) on the ALG1
  surface is `r round(day.sum[1,1],2)`.  This is the mean of the posterior distribution
  for this parameter.  If we back-transform this to the response scale, this
  becomes `r round(exp(day.sum[1, 1]),2)`.
- the estimated effect of ALG2 vs ALG1 is `r round(day.sum[2,1],2)` (mean) or 
  `r round(day.sum[2,4],2)` (median) with a standard deviation of `r round(day.sum[2,2],2)`.
  The 90% credibility intervals indicate that we are 90% confident that the slope is between 
  `r round(day.sum[2,1],2)` and `r round(day.sum[2,5],2)` - e.g. there is a
  significant positive effect.  When back-transformed onto the response scale, we see
  that barnacle recruitment on ALG2 is `r round(exp(day.sum[2,1]),2)` times
  higher than that on ALG1.
  This represents a `r round(100*(exp(day.sum[2, 1])-1), 0)`% increase in
  barnacle recruitment.
- the estimated effect of NB and S are `r round(day.sum[3, 1], 2)` and 
  `r round(day.sum[4, 1], 2)` respectively, which equate to 
  `r round(1/exp(day.sum[3,1]),2)` and  `r round(1/exp(day.sum[4,1]),2)` fold
  declines respectively.
- Rhat and number of effective samples for each parameter are also provided as
  MCMC diagnostics and all look good.


### tidyMCMC

```{r summariseModel1b, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=8, fig.height=5}
tidyMCMC(day.rstanarm3$stanfit, estimate.method='median',  conf.int=TRUE,  conf.method='HPDinterval',  rhat=TRUE, ess=TRUE)
```
```{r summariseModel1b1, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=8, fig.height=5,echo=FALSE}
day.tidy <- tidyMCMC(day.rstanarm3$stanfit, estimate.method='median',  conf.int=TRUE,  conf.method='HPDinterval',  rhat=TRUE, ess=TRUE)
```

**Conclusions:**


- the estimated mean (expected number of newly recruited barnacles) on the ALG1
  surface is `r round(as.numeric(day.tidy[1,2]),2)`.  This is the mean of the posterior distribution
  for this parameter.  If we back-transform this to the response scale, this
  becomes `r round(exp(as.numeric(day.tidy[1, 2])),2)`.
- the estimated effect of ALG2 vs ALG1 is `r round(as.numeric(day.tidy[2,2]),2)`
(median) with a standard error of `r round(as.numeric(day.tidy[2,3]),2)`.
  The 95% credibility intervals indicate that we are 95% confident that the effect is between 
  `r round(as.numeric(day.tidy[2,4]),2)` and `r round(as.numeric(day.tidy[2,5]),2)` - e.g. there is a
  significant positive effect.  When back-transformed onto the response scale, we see
  that barnacle recruitment on ALG2 is `r round(exp(as.numeric(day.tidy[2,2])),2)` times
  higher than that on ALG1.
  This represents a `r round(100*(exp(as.numeric(day.tidy[2, 2]))-1), 0)`% increase in
  barnacle recruitment.
 - the estimated effect of NB and S are `r round(as.numeric(day.tidy[3, 2]), 2)` and 
  `r round(as.numeric(day.tidy[4, 2]), 2)` respectively, which equate to 
  `r round(1/exp(as.numeric(day.tidy[3,2])),2)` and  `r round(1/exp(as.numeric(day.tidy[4,2])),2)` fold
  declines respectively. 
- Rhat and number of effective samples for each parameter are also provided as
  MCMC diagnostics and all look good.


### summarise_draws (posterior)

```{r summariseModel1dd, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=8, fig.height=5}
day.rstanarm3$stanfit |>
    summarise_draws(median,
                    HDInterval::hdi,
                    rhat, length, ess_bulk, ess_tail)
```

We can also alter the CI level.

```{r summariseModel1d2, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=8, fig.height=5}
day.rstanarm3$stanfit |>
    summarise_draws(median,
                    ~HDInterval::hdi(.x, credMass = 0.9),
                    rhat, length, ess_bulk, ess_tail)
```

Arguably, it would be better to back-transform to the ratio scale

```{r summariseModel1d3, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=8, fig.height=5}
day.rstanarm3$stanfit |>
    summarise_draws(
        ~ median(exp(.x)),
        ~HDInterval::hdi(exp(.x)),
        rhat, length, ess_bulk, ess_tail)
```

### as_draws_df (posteriors)

```{r summariseModel1m, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=8, fig.height=5}
day.rstanarm3$stanfit |> as_draws_df()
day.rstanarm3$stanfit |>
  as_draws_df() |>
  summarise_draws(
    median,
    ~ HDInterval::hdi(.x),
    rhat,
    length,
    ess_bulk, ess_tail
  )

day.rstanarm3$stanfit |>
    as_draws_df() |>
    exp() |>
    summarise_draws(
        median,
        ~ HDInterval::hdi(.x),
        rhat,
        length,
        ess_bulk, ess_tail
    )
```

### gather_draws

Due to the presence of a log transform in the predictor, it is better to use the
regex version.
```{r summariseModel1c, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=8, fig.height=5}
day.rstanarm3 |> get_variables()
day.draw <- day.rstanarm3 |> gather_draws(`.Intercept.*|.*TREAT.*`,  regex=TRUE)
day.draw

exceedP <- function(x, Val = 0) mean(x>Val)

day.rstanarm3 |>
    gather_draws(`.Intercept.*|.*TREAT.*`,  regex=TRUE) |>
    mutate(.value = exp(.value)) |>
    summarise_draws(median,
                    HDInterval::hdi,
                    rhat,
                    length,
                    ess_bulk,
                    ess_tail,
                    ~ exceedP(.x, 1))
```

We can then summarise this

```{r summariseModel1c1, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=8, fig.height=5}
day.draw |> median_hdci(.value)
```
We could alternatively express the parameters on the response scale.
```{r summariseModel1c8, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=8, fig.height=5}
day.draw |> median_hdci(exp(.value))
```

```{r summariseModel1c3, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=8, fig.height=5,echo=FALSE}
day.gather <- day.rstanarm3 |> gather_draws(`.Intercept.*|.*TREAT.*`,  regex=TRUE) |>
  median_hdci()
```

**Conclusions:**

- the estimated mean (expected number of newly recruited barnacles) on the ALG1
  surface is `r round(as.numeric(day.gather[1,2]),2)`.  This is the mean of the posterior distribution
  for this parameter.  If we back-transform this to the response scale, this
  becomes `r round(exp(as.numeric(day.gather[1, 2])),2)`.
- the estimated effect of ALG2 vs ALG1 is `r round(as.numeric(day.gather[2,2]),2)`
(median) with a standard error of `r round(as.numeric(day.gather[2,3]),2)`.
  The 95% credibility intervals indicate that we are 95% confident that the effect is between 
  `r round(as.numeric(day.gather[2,4]),2)` and `r round(as.numeric(day.gather[2,5]),2)` - e.g. there is a
  significant positive effect.  When back-transformed onto the response scale, we see
  that barnacle recruitment on ALG2 is `r round(exp(as.numeric(day.gather[2,2])),2)` times
  higher than that on ALG1.
  This represents a `r round(100*(exp(as.numeric(day.gather[2, 2]))-1), 0)`% increase in
  barnacle recruitment.
 - the estimated effect of NB and S are `r round(as.numeric(day.gather[3, 2]), 2)` and 
  `r round(as.numeric(day.gather[4, 2]), 2)` respectively, which equate to 
  `r round(1/exp(as.numeric(day.gather[3,2])),2)` and  `r round(1/exp(as.numeric(day.gather[4,2])),2)` fold
  declines respectively. 
- Rhat and number of effective samples for each parameter are also provided as
  MCMC diagnostics and all look good.


### bayesplot

```{r summariseModel1j, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=8, fig.height=5}
day.rstanarm3 |> plot(plotfun='mcmc_intervals') 
```

### half-eye (ggdist)

```{r summariseModel1c4, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=8, fig.height=5,echo=TRUE}
day.rstanarm3 |> 
  gather_draws(`.Intercept.*|.*TREAT.*`, regex=TRUE) |> 
  ggplot() + 
  stat_halfeye(aes(x=.value,  y=.variable)) +
  facet_wrap(~.variable, scales='free')

day.rstanarm3 |> 
  gather_draws(`.*TREAT.*`, regex=TRUE) |> 
  ggplot() + 
    stat_halfeye(aes(x=.value,  y=.variable)) +
    geom_vline(xintercept = 0, linetype = 'dashed')

day.rstanarm3 |> 
  gather_draws(`.*TREAT.*`, regex=TRUE) |> 
  ggplot() + 
    stat_halfeye(aes(x=exp(.value),  y=.variable)) +
    geom_vline(xintercept = 1, linetype = 'dashed') +
    scale_x_continuous(trans = scales::log2_trans())
```

### density ridges (ggridges)

```{r summariseModel1c7, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=8, fig.height=5,echo=TRUE}
day.rstanarm3 |> 
  gather_draws(`.*TREAT.*`, regex=TRUE) |> 
  ggplot() + 
    geom_density_ridges(aes(x=.value, y = .variable), alpha=0.4) +
    geom_vline(xintercept = 0, linetype = 'dashed')
##Or on a fractional scale
day.rstanarm3 |> 
  gather_draws(`.*TREAT.*`, regex=TRUE) |> 
  ggplot() + 
    geom_density_ridges_gradient(aes(x=exp(.value),
                                     y = .variable,
                                     fill = stat(x)),
                                 alpha=0.4, colour = 'white',
                                 quantile_lines = TRUE,
                                 quantiles = c(0.025, 0.975)) +
    geom_vline(xintercept = 1, linetype = 'dashed') +
    scale_x_continuous(trans = scales::log2_trans()) +
    scale_fill_viridis_c(option = "C")
```
 

### tidy_draws

This is purely a graphical depiction on the posteriors.

```{r summariseModel1d, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=8, fig.height=5}
day.rstanarm3 |> tidy_draws()
```

### spread_draws

```{r summariseModel1e, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=8, fig.height=5}
day.rstanarm3 |> spread_draws(`.Intercept.*|.*TREAT.*`,  regex=TRUE)
```

### posterior_samples
```{r summariseModel1f, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=8, fig.height=5}
day.rstanarm3 |> posterior_samples() |> as_tibble()
```

### $R^2$

Unfortunately, $R^2$ calculations for models other than Gaussian and Binomial
have not yet been implemented for `rstanarm` models yet.

```{r summariseModel1g, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=8, fig.height=5}
#day.rstanarm3 |> bayes_R2() |> median_hdci
```
::::

## brms 
:::: {.panel-tabset}

`brms` captures the MCMC samples from `stan` within the returned list.
There are numerous ways to retrieve and summarise these samples.  The first
three provide convenient numeric summaries from which you can draw conclusions,
the last four provide ways of obtaining the full posteriors. 

### summary

The `summary()` method generates simple summaries (mean, standard deviation as
well as 10, 50 and 90 percentiles).

```{r summariseModel2a, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=8, fig.height=5}
day.brm3 |> summary()
```

```{r summariseModel2a1, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=8, fig.height=5, echo=FALSE}
day.sum <- summary(day.brm3)
```

**Conclusions:**

- in the Model Info, we are informed that the total MCMC posterior sample size
  is `r nrow(as.matrix(day.rstanarm3))` and that there were 20 raw observations.
- the estimated mean (expected number of newly recruited barnacles) on the ALG1
  surface is `r round(day.sum$fixed[1,1],2)`.  This is the mean of the posterior distribution
  for this parameter.  If we back-transform this to the response scale, this
  becomes `r round(exp(day.sum$fixed[1, 1]),2)`.
- the estimated effect of ALG2 vs ALG1 is `r round(day.sum$fixed[2,1],2)` (mean) or 
  `r round(day.sum$fixed[2,4],2)` (median) with a standard deviation of `r round(day.sum$fixed[2,2],2)`.
  The 90% credibility intervals indicate that we are 90% confident that the slope is between 
  `r round(day.sum$fixed[2,1],2)` and `r round(day.sum$fixed[2,5],2)` - e.g. there is a
  significant positive effect.  When back-transformed onto the response scale, we see
  that barnacle recruitment on ALG2 is `r round(exp(day.sum$fixed[2,1]),2)` times
  higher than that on ALG1.
  This represents a `r round(100*(exp(day.sum$fixed[2, 1])-1), 0)`% increase in
  barnacle recruitment.
- the estimated effect of NB and S are `r round(day.sum$fixed[3, 1], 2)` and 
  `r round(day.sum$fixed[4, 1], 2)` respectively, which equate to 
  `r round(1/exp(day.sum$fixed[3,1]),2)` and  `r round(1/exp(day.sum$fixed[4,1]),2)` fold
  decline respectively.
- Rhat and number of effective samples for each parameter are also provided as
  MCMC diagnostics and all look good.

### tidyMCMC

```{r summariseModel2b, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=8, fig.height=5}
day.brm3$fit |> tidyMCMC(estimate.method = 'median',
                          conf.int = TRUE,
                          conf.method = 'HPDinterval',
                          rhat = TRUE,
                          ess = TRUE)
```
```{r summariseModel2b1, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=8, fig.height=5,echo=FALSE}
day.tidy <- tidyMCMC(day.brm3$fit, estimate.method='median',  conf.int=TRUE,  conf.method='HPDinterval',  rhat=TRUE, ess=TRUE)
```

**Conclusions:**


- the estimated mean (expected number of newly recruited barnacles) on the ALG1
  surface is `r round(as.numeric(day.tidy[1,2]),2)`.  This is the mean of the posterior distribution
  for this parameter.  If we back-transform this to the response scale, this
  becomes `r round(exp(as.numeric(day.tidy[1, 2])),2)`.
- the estimated effect of ALG2 vs ALG1 is `r round(as.numeric(day.tidy[2,2]),2)`
(median) with a standard error of `r round(as.numeric(day.tidy[2,3]),2)`.
  The 95% credibility intervals indicate that we are 95% confident that the effect is between 
  `r round(as.numeric(day.tidy[2,4]),2)` and `r round(as.numeric(day.tidy[2,5]),2)` - e.g. there is a
  significant positive effect.  When back-transformed onto the response scale, we see
  that barnacle recruitment on ALG2 is `r round(exp(as.numeric(day.tidy[2,2])),2)` times
  higher than that on ALG1.
  This represents a `r round(100*(exp(as.numeric(day.tidy[2, 2]))-1), 0)`% increase in
  barnacle recruitment.
 - the estimated effect of NB and S are `r round(as.numeric(day.tidy[3, 2]), 2)` and 
  `r round(as.numeric(day.tidy[4, 2]), 2)` respectively, which equate to 
  `r round(1/exp(as.numeric(day.tidy[3,2])),2)` and  `r round(1/exp(as.numeric(day.tidy[4,2])),2)` fold
  declines respectively. 
- Rhat and number of effective samples for each parameter are also provided as
  MCMC diagnostics and all look good.

### as_draws_df (posteriors)

```{r summariseModel2m, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=8, fig.height=5}
day.brm3 |> as_draws_df()
day.brm3 |>
  as_draws_df() |>
  summarise_draws(
    median,
    HDInterval::hdi,
    rhat,
    length,
    ess_bulk, ess_tail
  )

day.brm3 |>
  as_draws_df() |>
    exp() |>
  summarise_draws(
    median,
    HDInterval::hdi,
    rhat,
    length,
    ess_bulk, ess_tail
  )
```


### gather_draws

Due to the presence of a log transform in the predictor, it is better to use the
regex version.
```{r summariseModel2c, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=8, fig.height=5}
day.brm3 |> get_variables()
day.draw <- day.brm3 |>
    gather_draws(`.Intercept.*|.*TREAT.*`,  regex = TRUE)
day.draw

day.brm3 |>
    gather_draws(`.Intercept.*|.*TREAT.*`,  regex = TRUE) |>
    mutate(.value = exp(.value)) |>
    summarise_draws(median,
                    ~HDInterval::hdi(.x, credMass = 0.95),
                    rhat,
                    length,
                    ess_bulk, ess_tail)
    

exceedP <- function(x, Val = 0) mean(x>Val)
day.brm3 |>
    tidy_draws() |>
    exp() |>
    dplyr::select(starts_with("b_")) |>
    summarise_draws(median,
                    ~HDInterval::hdi(.x, credMass = 0.9),
                    rhat,
                    ess_bulk, ess_tail,
                    ~exceedP(.x, 1))
```

We can then summarise this

```{r summariseModel2c1, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=8, fig.height=5}
day.draw |> median_hdci(.value)
```

We could alternatively express the parameters on the response scale.

```{r summariseModel2c5, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=8, fig.height=5,echo=TRUE}
day.draw |> 
  median_hdci(exp(.value))
```


```{r summariseModel2c3, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=8, fig.height=5,echo=FALSE}
day.gather <- day.brm3 |> gather_draws(`.Intercept.*|.*TREAT.*`,  regex = TRUE) |>
  median_hdci()
```

```{r summariseModel2c4a, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=8, fig.height=5,echo=TRUE}
day.brm3 |> 
  gather_draws(`.Intercept.*|.*TREAT.*`, regex=TRUE) |> 
  ggplot() + 
  stat_halfeye(aes(x=.value,  y=.variable)) +
  facet_wrap(~.variable, scales='free')
```
 
**Conclusions:**

- the estimated mean (expected number of newly recruited barnacles) on the ALG1
  surface is `r round(as.numeric(day.gather[1,2]),2)`.  This is the mean of the posterior distribution
  for this parameter.  If we back-transform this to the response scale, this
  becomes `r round(exp(as.numeric(day.gather[1, 2])),2)`.
- the estimated effect of ALG2 vs ALG1 is `r round(as.numeric(day.gather[2,2]),2)`
(median) with a standard error of `r round(as.numeric(day.gather[2,3]),2)`.
  The 95% credibility intervals indicate that we are 95% confident that the effect is between 
  `r round(as.numeric(day.gather[2,4]),2)` and `r round(as.numeric(day.gather[2,5]),2)` - e.g. there is a
  significant positive effect.  When back-transformed onto the response scale, we see
  that barnacle recruitment on ALG2 is `r round(exp(as.numeric(day.gather[2,2])),2)` times
  higher than that on ALG1.
  This represents a `r round(100*(exp(as.numeric(day.gather[2, 2]))-1), 0)`% increase in
  barnacle recruitment.
 - the estimated effect of NB and S are `r round(as.numeric(day.gather[3, 2]), 2)` and 
  `r round(as.numeric(day.gather[4, 2]), 2)` respectively, which equate to 
  `r round(1/exp(as.numeric(day.gather[3,2])),2)` and  `r round(1/exp(as.numeric(day.gather[4,2])),2)` fold
  declines respectively. 
- Rhat and number of effective samples for each parameter are also provided as
  MCMC diagnostics and all look good.

### bayesplot

```{r summariseModel2j, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=8, fig.height=5}
day.brm3$fit |> plot(type='intervals') 
```

### slab (ggdist)

```{r summariseModel2d5, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=8, fig.height=5,echo=TRUE}
## Link scale
day.brm3 |>
    gather_draws(`.Intercept.*|.*TREAT.*`, regex=TRUE) |> 
    ggplot() +
    stat_slab(aes(x = .value, y = .variable,
                  fill = stat(ggdist::cut_cdf_qi(cdf,
                           .width = c(0.5, 0.8, 0.95), 
                           labels = scales::percent_format())
                           )), color='black') + 
    geom_vline(xintercept=0, linetype='dashed') +
    scale_fill_brewer('Interval', direction = -1, na.translate = FALSE) 
## Fractional scale
day.brm3 |>
    gather_draws(`.Intercept.*|.*TREAT.*`, regex=TRUE) |>
    mutate(.value=exp(.value)) |>
    ggplot() +
    stat_slab(aes(x = .value, y = .variable,
                  fill = stat(ggdist::cut_cdf_qi(cdf,
                           .width = c(0.5, 0.8, 0.95), 
                           labels = scales::percent_format())
                           )), color='black') + 
    geom_vline(xintercept=1, linetype='dashed') +
    scale_fill_brewer('Interval', direction = -1, na.translate = FALSE) +
    scale_x_continuous(trans = scales::log2_trans())
```

### half-eye (ggdist)

```{r summariseModel2c4, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=8, fig.height=5,echo=TRUE}
day.brm3 |> 
  gather_draws(`.Intercept.*|.*TREAT.*`, regex=TRUE) |> 
  ggplot() + 
  stat_halfeye(aes(x=.value,  y=.variable)) +
  facet_wrap(~.variable, scales='free')

day.brm3 |> 
  gather_draws(`.*TREAT.*`, regex=TRUE) |> 
  ggplot() + 
    stat_halfeye(aes(x=.value,  y=.variable)) +
    geom_vline(xintercept = 0, linetype = 'dashed')

day.brm3 |> 
  gather_draws(`.*TREAT.*`, regex=TRUE) |> 
  ggplot() + 
    stat_halfeye(aes(x=exp(.value),  y=.variable)) +
    geom_vline(xintercept = 1, linetype = 'dashed') +
    scale_x_continuous(trans = scales::log2_trans())
```

### ggridges

```{r summariseModel2c7, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=8, fig.height=5,echo=TRUE}
day.brm3 |> 
  gather_draws(`.*TREAT.*`, regex=TRUE) |> 
  ggplot() + 
    geom_density_ridges(aes(x=.value, y = .variable), alpha=0.4) +
    geom_vline(xintercept = 0, linetype = 'dashed')
##Or on a fractional scale
day.brm3 |> 
  gather_draws(`.*TREAT.*`, regex=TRUE) |> 
  ggplot() + 
    geom_density_ridges_gradient(aes(x=exp(.value),
                                     y = .variable,
                                     fill = stat(x)),
                                 alpha=0.4, colour = 'white',
                                 quantile_lines = TRUE,
                                 quantiles = c(0.025, 0.975)) +
    geom_vline(xintercept = 1, linetype = 'dashed') +
    scale_x_continuous(trans = scales::log2_trans()) +
    scale_fill_viridis_c(option = "C")
```
 

### tidy_draws

This is purely a graphical depiction on the posteriors.

```{r summariseModel2d, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=8, fig.height=5}
day.brm3 |> tidy_draws()
```

### spread_draws

```{r summariseModel2e, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=8, fig.height=5}
day.brm3 |> spread_draws(`.Intercept.*|.*TREAT.*`,  regex=TRUE)
```

### posterior_samples
```{r summariseModel2f, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=8, fig.height=5}
day.brm3 |> posterior_samples() |> as_tibble()
```

### $R^2$

```{r summariseModel2g, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=8, fig.height=5}
day.brm3 |> bayes_R2(summary=FALSE) |> median_hdci()
```

### ROPE

Region of Practical Equivalence

```{r summariseModel2k, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=8, fig.height=5}
0.1 * sd(log(day$BARNACLE))
day.brm3 |> rope(range = c(-0.04, 0.04))
rope(day.brm3, range = c(-0.04, 0.04)) |> plot()

## Or based on fractional scale
day.brm3 |>
    as_draws_df('^b_TREAT.*', regex = TRUE) |>
    exp() |> 
    ## equivalence_test(range = c(0.9, 1.1))
    rope(range = c(0.9, 1.1))
day.mcmc <-
    day.brm3 |> as_draws_df('^b_TREAT.*', regex = TRUE) |>
    exp()
day.mcmc |>
    rope(range = c(0.9, 1.1))
day.mcmc |>
    rope(range = c(0.9, 1.1)) |>
    plot(day.mcmc)

day.mcmc |>
    equivalence_test(range = c(0.9, 1.1)) 
```

::::
:::
<!-- END_PRIVATE-->

# Further investigations 
<!-- START_PRIVATE-->
::: {.panel-tabset}

The estimated coefficients as presented in the summary tables above highlight
very specific comparisons.  However, there are other possible comparisons that
we might be interested in.  For example, whist the treatment effects compare
each of the substrate types against the first level (ALG1), we might also be
interested in the differences between (for example) the two bare substrates (NB
and S).

To get at more of the comparisons we have two broad approaches:

- compare all substrates to each other in a pairwise manner.
   - importantly, given that the probability of a Type I error (falsely rejecting a
  null hypothesis) is set at 0.05 per comparison, when there are multiple
  comparisons, the family-wise Type I error (probability of at least one false
  rejection amongst all comparisons) rate compounds.  It is important to
  attempt to constrain this family-wise Type I error rate.  One approach to do
  this is a Tukey's test.
   - keep in mind that in order to constrain the family-wise Type I error rate at
  0.05, the power of each individual comparison is reduced (individual Type II
  error rates increased).
- define a small set of specific comparisons.  There should be a maximum of
  $p-1$ comparisons defined (where $p$ is the number of levels of the
  categorical predictors) and each comparison should be independent of one another.

We will now only peruse the Poisson model.

## Post-hoc test (Tukey's)

:::: {.panel-tabset}

### rstanarm 

::::: {.panel-tabset}
#### effects

```{r Probability1a, results='markdown', echo=1,eval=TRUE, mhidden=TRUE, fig.width=8, fig.height=5}
day.rstanarm3 |> emmeans(pairwise ~TREAT, type='response')
day.pairwise <- (day.rstanarm3 |>
  emmeans(pairwise ~TREAT, type='response') |>
  confint())$contrasts |>
  as.data.frame()
```

**Conclusions:**

- the contrasts section of the output indicates that there is evidence of:
- ALG1 has `r round(day.pairwise[2, 2], 2)` fold
  (`r 100*(round(day.pairwise[2, 2], 2)-1)`%) more newly recruited barnacles
  than the NB substrate
- ALG1 has `r round(day.pairwise[3, 2], 2)` fold
  (`r 100*(round(day.pairwise[3, 2], 2)-1)`%) more newly recruited barnacles
  than the S substrate
- ALG2 has `r round(day.pairwise[4, 2], 2)` fold
  (`r 100*(round(day.pairwise[4, 2], 2)-1)`%) more newly recruited barnacles
  than the NB substrate
- ALG2 has `r round(day.pairwise[5, 2], 2)` fold
  (`r 100*(round(day.pairwise[5, 2], 2)-1)`%) more newly recruited barnacles
  than the S substrate
- ALG1 was not found to be different to ALG2 and NB was not found to be
  different to S  

#### Probabilities

```{r Probability1b, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=8, fig.height=5}
day.em = emmeans(day.rstanarm3, pairwise~TREAT, type='link')$contrasts |>
    gather_emmeans_draws() |>
    mutate(Fit=exp(.value))
day.em |> head()
day.em |> group_by(contrast) |>
    ggplot(aes(x=Fit)) +
    geom_histogram() +
    geom_vline(xintercept=1, color='red') + 
    facet_wrap(~contrast, scales='free')
day.em |> group_by(contrast) |>
    ggplot(aes(x=Fit)) +
    geom_density_ridges_gradient(aes(y = contrast, fill = stat(x)),
                                 alpha = 0.4, color = 'white',
                                 quantile_lines = TRUE,
                                 quantiles = c(0.025, 0.975)) +
    geom_vline(xintercept=1, linetype = 'dashed') +
    scale_fill_viridis_c(option = "C") +
    scale_x_continuous(trans = scales::log2_trans())

day.em |> 
  ggplot(aes(x = Fit)) + 
    geom_density_ridges_gradient(aes(y = contrast,
                                     fill = factor(stat(x>0))),
                                 alpha=0.4, colour = 'white',
                                 quantile_lines = TRUE,
                                 quantiles = c(0.025, 0.975)) +
    geom_vline(xintercept = 1, linetype = 'dashed') +
    scale_x_continuous(trans = scales::log2_trans()) +
    scale_fill_viridis_d()

day.em |> group_by(contrast) |> median_hdi()
# Probability of effect
day.em |> group_by(contrast) |> summarize(P=mean(Fit>1))
day.em |> group_by(contrast) |> summarize(P=mean(Fit<1))
##Probability of effect greater than 10%
day.em |> group_by(contrast) |> summarize(P=mean(Fit>1.1))
```
:::::
### brms 
::::: {.panel-tabset}
	
#### effects

```{r Probability2a, results='markdown', echo=TRUE,eval=TRUE, mhidden=TRUE, fig.width=8, fig.height=5}
day.brm3 |>
    emmeans(~TREAT, type = 'response') |>
    pairs()
#OR
day.pairwise <- day.brm3 |>
    emmeans(~TREAT, type = 'response') |>
    pairs() |>
    as.data.frame()
##OR
day.brm3 |>
    emmeans(~TREAT) |>
    pairs() |>
    tidy_draws() |>
    exp() |>
    summarise_draws(median)
##OR
day.brm3 |>
    emmeans(~TREAT) |>
    pairs() |>
    gather_emmeans_draws() |>
    mutate(.ratio = exp(.value)) |>
    ## median_hdci(.ratio)
    summarise(
        median_hdci(.ratio),
        P = mean(.ratio>1),
        ROPE = rope(.ratio, range = c(0.9, 1.1))$ROPE_Percentage)
    ## summarise(across(c(.value, .ratio), c(median, HDInterval::hdi)))
    ## summarise(across(c(.value, .ratio), c(median_hdci)))

day.mcmc <-
    day.brm3 |>
    emmeans(~TREAT) |>
    pairs() |>
    tidy_draws() |>
    dplyr::select(-.chain, -.iteration, -.draw) |>
    exp() 
rope(day.mcmc, range = c(0.9, 1.1)) |> plot()
```

**Conclusions:**

- the contrasts section of the output indicates that there is evidence of:
- ALG1 has `r round(day.pairwise[2, 2], 2)` fold
  (`r 100*(round(day.pairwise[2, 2], 2)-1)`%) more newly recruited barnacles
  than the NB substrate
- ALG1 has `r round(day.pairwise[3, 2], 2)` fold
  (`r 100*(round(day.pairwise[3, 2], 2)-1)`%) more newly recruited barnacles
  than the S substrate
- ALG2 has `r round(day.pairwise[4, 2], 2)` fold
  (`r 100*(round(day.pairwise[4, 2], 2)-1)`%) more newly recruited barnacles
  than the NB substrate
- ALG2 has `r round(day.pairwise[5, 2], 2)` fold
  (`r 100*(round(day.pairwise[5, 2], 2)-1)`%) more newly recruited barnacles
  than the S substrate
- ALG1 was not found to be different to ALG2 and NB was not found to be
  different to S  

#### Probabilities

```{r Probability2b, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=8, fig.height=5}
day.brm3 |> emmeans(~TREAT, type = 'response') |>
    pairs()
day.em <- day.brm3 |>
    emmeans(~TREAT, type = 'link') |>
    pairs() |>
    gather_emmeans_draws() |>
    mutate(Fit = exp(.value))
day.em |> group_by(contrast) |>
    ggplot(aes(x=Fit)) +
    geom_density_ridges_gradient(aes(y = contrast, fill = stat(x)),
                                 alpha = 0.4, color = 'white',
                                 quantile_lines = TRUE,
                                 quantiles = c(0.025, 0.975)) +
    geom_vline(xintercept=1, linetype = 'dashed') +
    scale_fill_viridis_c(option = "C") +
    scale_x_continuous(trans = scales::log2_trans())

day.em |> 
  ggplot(aes(x = Fit)) + 
    geom_density_ridges_gradient(aes(y = contrast,
                                     fill = factor(stat(x>0))),
                                 alpha=0.4, colour = 'white',
                                 quantile_lines = TRUE,
                                 quantiles = c(0.025, 0.975)) +
    geom_vline(xintercept = 1, linetype = 'dashed') +
    scale_x_continuous(trans = scales::log2_trans()) +
    scale_fill_viridis_d() +
    scale_x_continuous(trans = scales::log2_trans())

day.em |> head()
day.em |>
    group_by(contrast) |>
    ggplot(aes(x = Fit)) +
    ## geom_histogram() +
    geom_halfeyeh() +
    geom_vline(xintercept = 1, color = 'red') + 
    facet_wrap(~contrast, scales = 'free')
day.em |>
    group_by(contrast) |>
    median_hdi()
# Probability of effect
day.em |> group_by(contrast) |> summarize(P = mean(Fit>1))
##Probability of effect greater than 10%
day.em |> group_by(contrast) |> summarize(P = mean(Fit>1.1))

## Effect size on absolute scale
day.em <- day.brm3 |>
    emmeans(~TREAT, type = 'link') |>
    regrid() |>
    pairs() |>
    gather_emmeans_draws() |>
    median_hdci(.value)

day.brm3 |>
    emmeans(~TREAT, type = 'link') |>
    regrid() |>
    pairs() |>
    gather_emmeans_draws() |>
    group_by(contrast) |>
    ggplot(aes(x = .value)) +
    geom_halfeyeh() +
    geom_vline(xintercept = 0, color = 'red') + 
    facet_wrap(~contrast, scales = 'free')


day.brm3 |>
    emmeans(~TREAT, type = 'response') |>
    pairs() |>
    gather_emmeans_draws() |>
    mutate(.value = exp(.value)) |>
    ggplot(aes(x = .value)) + 
    geom_density_ridges_gradient(aes(y = contrast,
                                     fill = factor(stat(x>0))),
                                 alpha=0.4, colour = 'white',
                                 quantile_lines = TRUE,
                                 quantiles = c(0.025, 0.975)) +
    geom_vline(xintercept = 1, linetype = 'dashed') +
    scale_x_continuous(trans = scales::log2_trans()) +
    scale_fill_viridis_d() 

```
:::::
::::

## Planned contrasts

Define your own

Compare:

a) ALG1 vs ALG2
b) NB vs S
c) average of ALG1+ALG2 vs NB+S


| Levels | Alg1 vs Alg2 | NB vs S | Alg vs Bare |
| ------ | ------------ | ------- | ----------- |
| Alg1   | 1            | 0       | 0.5         |
| Alg2   | -1           | 0       | 0.5         |
| NB     | 0            | 1       | -0.5        |
| S      | 0            | -1      | -0.5        |

:::: {.panel-tabset}

### rstanarm 

```{r Probability1c, results='markdown', echo=TRUE, eval=TRUE, mhidden=TRUE, fig.width=8, fig.height=5}
##Planned contrasts
cmat<-cbind('Alg2_Alg1'=c(-1,1,0,0),
              'NB_S'=c(0,0,1,-1),
             'Alg_Bare'=c(0.5,0.5,-0.5,-0.5),
             'Alg_NB'=c(0.5,0.5,-1,0))
# On the link scale
emmeans(day.rstanarm3, ~TREAT, contr=list(TREAT=cmat), type='link')
# On the response scale
emmeans(day.rstanarm3, ~TREAT, contr=list(TREAT=cmat), type='response')

day.em = emmeans(day.rstanarm3, ~TREAT, contr=list(TREAT=cmat), type='link')$contrasts |>
      gather_emmeans_draws() |> mutate(Fit=exp(.value)) 
day.em |> group_by(contrast) |> mean_hdi()
# Probability of effect
day.em |> group_by(contrast) |> summarize(P=mean(Fit>1))
##Probability of effect greater than 10%
day.em |> group_by(contrast) |> summarize(P=mean(Fit>1.5))


day.sum <- day.em |>
  group_by(contrast) |>
  median_hdci(.width=c(0.8, 0.95))
day.sum
ggplot(day.sum) +
  geom_hline(yintercept=1, linetype='dashed') +
  geom_pointrange(aes(x=contrast, y=Fit, ymin=Fit.lower, ymax=Fit.upper, size=factor(.width)),
                  show.legend = FALSE) +
  scale_size_manual(values=c(1, 0.5)) +
  coord_flip()

g1 <- ggplot(day.sum) +
  geom_hline(yintercept=1) +
  geom_pointrange(aes(x=contrast, y=Fit, ymin=Fit.lower, ymax=Fit.upper, size=factor(.width)), show.legend = FALSE) +
  scale_size_manual(values=c(1, 0.5)) +
  scale_y_continuous(trans=scales::log2_trans(),  breaks=c(0.5, 1, 2, 4)) +
  coord_flip() + 
  theme_classic()
g1   
```

### brms 

```{r Probability2c, results='markdown', echo=TRUE, eval=TRUE, mhidden=TRUE, fig.width=8, fig.height=5}
##Planned contrasts
cmat<-cbind('Alg2_Alg1' = c(-1,1,0,0),
            'NB_S' = c(0,0,1,-1),
            'Alg_Bare' = c(0.5,0.5,-0.5,-0.5),
            'Alg_NB' = c(0.5,0.5,-1,0))
# On the link scale
day.brm3 |> emmeans(~TREAT, type = 'link') |>
    contrast(method = list(TREAT = cmat))
# On the response scale
day.brm3 |>
    emmeans(~TREAT, type = 'response') |>
    contrast(method = list(TREAT = cmat))

day.em <- day.brm3 |>
    emmeans(~TREAT, type = 'link') |>
    contrast(method = list(TREAT = cmat)) |>
    gather_emmeans_draws() |>
    mutate(Fit = exp(.value)) 
day.em |> median_hdi(Fit)
# Probability of effect
day.em |> summarize(P = mean(Fit>1))
##Probability of effect greater than 50%
day.em |> summarize(P = mean(Fit>1.5))

day.sum <- day.em |>
  group_by(contrast) |>
  median_hdci(.value, .width = c(0.8, 0.95))
day.sum
g1 <- ggplot(day.sum) +
  geom_vline(xintercept = 0, linetype = 'dashed') +
  geom_pointrange(aes(y = contrast, x = .value, xmin = .lower, xmax = .upper,
                      size = factor(.width)),
                  show.legend  =  FALSE) +
  scale_size_manual(values = c(1, 0.5)) 

day.sum <- day.em |>
  group_by(contrast) |>
  median_hdci(Fit, .width = c(0.8, 0.95))
day.sum
g1 <- ggplot(day.sum) +
  geom_vline(xintercept = 1, linetype='dashed') +
  geom_pointrange(aes(y = contrast, x = Fit, xmin = .lower, xmax = .upper, size = factor(.width)), show.legend = FALSE) +
  scale_size_manual(values = c(1, 0.5)) +
  scale_x_continuous(trans = scales::log2_trans(),  breaks = c(0.5, 0.8,1,1.2, 1.5, 2, 4)) +
  theme_classic()
g1   

g1a <- 
    day.em |>
    ggplot() +
    geom_vline(xintercept = 1, linetype = 'dashed') +
    #geom_vline(xintercept = 1.5, alpha=0.3, linetype = 'dashed') +
    stat_slab(aes(x = Fit, y = contrast,
                  fill = stat(ggdist::cut_cdf_qi(cdf,
                            .width = c(0.5, 0.8, 0.95), 
                            labels = scales::percent_format())
                            )), color = 'black') +
    scale_fill_brewer('Interval', direction  =  -1, na.translate = FALSE) +
    scale_x_continuous('Effect', trans = scales::log2_trans(),
                       breaks = c(0.5, 0.8,1,1.2, 1.5, 2, 4)) +
    scale_y_discrete('', breaks=c('TREAT.NB_S', 'TREAT.Alg2_Alg1', 'TREAT.Alg_NB', 'TREAT.Alg_Bare'),
                     labels = c('Nat. Bare vs Scapped', 'Algae 1 vs 2', 'Algae vs Nat. Bare', 'Algae vs Bare')) +
    theme_classic()
g1 + g1a

library(ggridges)
day.em |>
  ggplot() + 
    geom_density_ridges(aes(x=Fit, y = contrast), alpha=0.4) +
    geom_vline(xintercept = 1, linetype = 'dashed')
day.em |>
  ggplot() + 
    geom_density_ridges_gradient(aes(x=Fit,
                                     y = contrast,
                                     fill = stat(x)),
                                 alpha=0.4, colour = 'white',
                                 quantile_lines = TRUE,
                                 quantiles = c(0.025, 0.975)) +
    geom_vline(xintercept = 1, linetype = 'dashed') +
    scale_x_continuous(trans = scales::log2_trans()) +
    scale_fill_viridis_c(option = "C")
day.em |>
  ggplot() + 
    geom_density_ridges_gradient(aes(x=100*(Fit-1),
                                     y = contrast,
                                     fill = stat(x)),
                                 alpha=0.4, colour = 'white',
                                 quantile_lines = TRUE,
                                 quantiles = c(0.025, 0.975)) +
    geom_vline(xintercept = 1, linetype = 'dashed') +
    scale_x_continuous('Percentage change') +
    scale_fill_viridis_c(option = "C")
##Or on a fractional scale
day.brm3 |> 
  gather_draws(`.*TREAT.*`, regex=TRUE) |> 
  ggplot() + 
    geom_density_ridges_gradient(aes(x=exp(.value),
                                     y = .variable,
                                     fill = stat(x)),
                                 alpha=0.4, colour = 'white',
                                 quantile_lines = TRUE,
                                 quantiles = c(0.025, 0.975)) +
    geom_vline(xintercept = 1, linetype = 'dashed') +
    scale_x_continuous(trans = scales::log2_trans()) +
    scale_fill_viridis_c(option = "C")

```

::::
:::
<!-- END_PRIVATE-->

# Summary Figure

<!-- START_PRIVATE-->
::: {.panel-tabset}

## rstanarm 

```{r summaryFig1a, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=6, fig.height=3}
newdata = emmeans(day.rstanarm3, ~TREAT, type='response') |>
    as.data.frame()
newdata
## A quick version
g2 <- ggplot(newdata, aes(y=rate, x=TREAT)) +
    geom_pointrange(aes(ymin=lower.HPD, ymax=upper.HPD)) +
    theme_classic()
    
g2 + g1    
```

## brms 

```{r summaryFig2a, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=6, fig.height=3}
newdata <- day.brm3 |>
    emmeans(~TREAT, type='response') |>
    as.data.frame()
newdata
## A quick version
g2 <- ggplot(newdata, aes(y=rate, x=TREAT)) +
    geom_pointrange(aes(ymin=lower.HPD, ymax=upper.HPD)) +
    scale_y_continuous('Number of newly recruited barnacles') +
    scale_x_discrete('', breaks=c('ALG1', 'ALG2', 'NB', 'S'),
                     labels = c('Algae 1', 'Algae 2', 'Nat. Bare', 'Scraped')) +
    theme_classic()
    
g2 + g1    
(g2 + ggtitle('a)')) + (g1a + ggtitle('b)'))    
```

:::
<!-- END_PRIVATE-->

# References
