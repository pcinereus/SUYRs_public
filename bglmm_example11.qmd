---
title: "Bayesian GLM Part11"
author: "Murray Logan"
date: today
date-format: "DD/MM/YYYY"
format: 
  html:
    ## Format
    theme: [default, ../resources/ws-style.scss]
    css: ../resources/ws_style.css
    html-math-method: mathjax
    ## Table of contents
    toc: true
    toc-float: true
    ## Numbering
    number-sections: true
    number-depth: 3
    ## Layout
    page-layout: full
    fig-caption-location: "bottom"
    fig-align: "center"
    fig-width: 4
    fig-height: 4
    fig-dpi: 72
    tbl-cap-location: top
    ## Code
    code-fold: false
    code-tools: true
    code-summary: "Show the code"
    code-line-numbers: true
    code-block-border-left: "#ccc"
    code-copy: true
    highlight-style: atom-one
    ## Execution
    execute:
      echo: true
      cache: true
    ## Rendering
    embed-resources: true
crossref:
  fig-title: '**Figure**'
  fig-labels: arabic
  tbl-title: '**Table**'
  tbl-labels: arabic
engine: knitr
output_dir: "docs"
documentclass: article
fontsize: 12pt
mainfont: Arial
mathfont: LiberationMono
monofont: DejaVu Sans Mono
classoption: a4paper
bibliography: ../resources/references.bib
---

```{r}
#| label: setup
#| include: false

knitr::opts_chunk$set(cache.lazy = FALSE,
                      tidy = "styler")
options(tinytex.engine = "xelatex")
```


# Preparations

Load the necessary libraries

```{r}
#| label: libraries
#| output: false
#| eval: true
#| warning: false
#| message: false
#| cache: false

library(tidyverse)     #for data wrangling etc
library(rstanarm)      #for fitting models in STAN
library(cmdstanr)      #for cmdstan
library(brms)          #for fitting models in STAN
library(standist)      #for exploring distributions
library(coda)          #for diagnostics
library(bayesplot)     #for diagnostics
library(ggmcmc)        #for MCMC diagnostics
library(DHARMa)        #for residual diagnostics
library(rstan)         #for interfacing with STAN
library(emmeans)       #for marginal means etc
library(broom)         #for tidying outputs
library(tidybayes)     #for more tidying outputs
library(HDInterval)    #for HPD intervals
library(ggeffects)     #for partial plots
library(broom.mixed)   #for summarising models
library(posterior)     #for posterior draws
library(ggeffects)     #for partial effects plots
library(patchwork)     #for multi-panel figures
library(bayestestR)    #for ROPE
library(see)           #for some plots
library(easystats)     #framework for stats, modelling and visualisation
library(geoR)     #framework for stats, modelling and visualisation
library(modelsummary)  #for data and model summaries 
theme_set(theme_grey()) #put the default ggplot theme back
source('helperFunctions.R')
```

# Scenario

@Mitchell-2019 presented a data set in which male guppy fish were
placed individually into long, narrow and shallow tanks that
predominately only permitted the fish to swim in two dimensions. The
activity rate (cumulative distance swam) of each guppy fish was
remotely recorded via tracking software during 20 minute trials
conducted each day for 14 days. The order in which individual guppy
fish were recorded each day was randomised and the time of day of each
trial was recorded.

The researchers were interested in whether the the guppy fish become
habituated to the experimental conditions and altered their activity
patterns over time.

![guppy fish](../resources/guppy.png){#fig-guppy width=70%}

The data are in the file
**mitchell.csv** in the **data** folder.

| ID   | TOD  | Day | Dist_moved |
|------|------|-----|------------|
| A39  | 0.39 | 7   | 3439.93    |
| A3   | 0.39 | 7   | 2795.08    |
| A13  | 0.39 | 7   | 5258.79    |
| \... | \... |     |            |

: Format of the mitchell.csv data file {#tbl-mitchell .table-condensed}

---------------- ---------------------------------------------------
**ID**:           Individual guppy fish ID - Random variable
**TOD**:          Time of day of observation as a proportion - Predictor variable
**Day**:          Day post entry into tank - Predictor variable
**Dist_moved**:   Total distance (cm) moved in 20 min trial - Response variable
---------------- ---------------------------------------------------
 
: Description of the variables in the mitchell data file {#tbl-mitchell1 .table-condensed}
 
# Read in the data

```{r readData, results='markdown', eval=TRUE}
mitchell <- read_csv("../data/mitchell.csv", trim_ws = TRUE)
```

<!-- START_PRIVATE-->
::: {.panel-tabset}

## glimpse
```{r}
#| label: examinData
glimpse(mitchell)
```

## head
```{r}
## Explore the first 6 rows of the data
head(mitchell)
```

## str
```{r}
str(mitchell)
```

## Easystats (datawizard)
```{r}
mitchell |> datawizard::data_codebook()
```

## Skim (modelsummary)
```{r}
mitchell |> modelsummary::datasummary_skim()
mitchell |> modelsummary::datasummary_skim(by = "Day")
```

:::
<!-- END_PRIVATE-->

# Data preparation
<!-- START_PRIVATE--> 
Let start by declaring the categorical variables
and random effect as factors.


```{r dataProcessing, results='markdown', eval=TRUE, mhidden=TRUE}
mitchell <- mitchell |>
  mutate(ID = factor(ID),
    fDay = factor(Day))
```
<!-- END_PRIVATE-->

# Exploratory data analysis

<!-- START_PRIVATE--> As the response represents a strictly positive
real number (total distance moved), it would make sense to start by
considering a Gamma model.

Note, also that the response variable are relatively large distances
(as the units of distance used were centimeters). It might be worth
dividing these values by 100 or even 1000 during the modelling
process.

<!-- END_PRIVATE-->
Model formula:
$$
\begin{align}
y_i/1000 &\sim{} \mathcal{Gamma}(\lambda_i)\\
ln(\lambda_i) &=\boldsymbol{\beta} \bf{X_i} + \boldsymbol{\gamma} \bf{Z_i}
\end{align}
$$

where $\boldsymbol{\beta}$ and $\boldsymbol{\gamma}$ are vectors of
the fixed and random effects parameters respectively and $\bf{X}$ is
the model matrix representing the overall intercept and effects of day
and time of day on the total distance moved during the trial. $\bf{Z}$
represents a cell means model matrix for the random intercepts
associated with individual guppies.

<!-- START_PRIVATE-->
::: {.panel-tabset}
## Boxplots
```{r eda0, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=7, fig.height=5}
mitchell |> ggplot(aes(y = Dist_moved)) +
  geom_boxplot()
## mitchell |> ggplot(aes(y = Dist)) +
##   geom_boxplot()
mitchell |> ggplot(aes(y = Dist_moved)) +
  geom_boxplot() +
  scale_y_log10()
mitchell |> ggplot(aes(y = TOD)) +
  geom_boxplot() +
  scale_y_log10()
mitchell |> ggplot(aes(y = Day)) +
  geom_boxplot() 
```
## Scatter plots
```{r eda1, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=7, fig.height=5}
## mitchell |> ggplot(aes(y = Dist, x = Day)) +
##   geom_point() +
##   geom_line(aes(group = ID))
mitchell |> ggplot(aes(y = Dist_moved, x = TOD)) +
  geom_point() +
  geom_line(aes(group = ID))
```
## Facet plots
```{r eda2, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=15, fig.height=15}
mitchell |> ggplot(aes(y = Dist_moved, x = Day)) +
  geom_point() +
  scale_y_log10() +
  #scale_x_log10() +
  geom_smooth(method =  "lm") +
  facet_wrap(~ID)
mitchell |> ggplot(aes(y = Dist_moved, x = TOD)) +
  geom_point() +
  scale_y_log10() +
  #scale_x_log10() +
  geom_smooth(method =  "lm") +
  facet_wrap(~ID)

mitchell |> ggplot(aes(y = Day, x = TOD)) +
  geom_point() 
```
:::
<!-- END_PRIVATE-->


To reduce compute time, we will reduce the data set to a selection of
ID's

```{r}
#| label: prepare_data
#| results: markup
#| eval: true
#| echo: true
#| cache: false
mitchell <- mitchell |>
  filter(ID %in% c("B80", "B60", "B58", "B34", "B30", "B28")) |>
    droplevels()
mitchell |> ggplot(aes(y = Dist_moved, x = Day)) +
  geom_point() +
  scale_y_log10() +
  scale_x_log10() +
  geom_smooth(method =  "lm") +
  facet_wrap(~ID)
```


# Fit the model 

<!-- START_PRIVATE-->
::: {.panel-tabset}
## brms 
:::: {.panel-tabset}
### Using default priors

In `brms`, the default priors are designed to be weakly informative.  They are
chosen to provide moderate regularisation (to help prevent over fitting) and
help stabilise the computations.

Unlike `rstanarm`, `brms` models must be compiled before they start sampling.
For most models, the compilation of the stan code takes around 45 seconds.

```{r fitModel2a, results='markdown', eval=TRUE, mhidden=TRUE, paged.print=FALSE, tidy.opts = list(width.cutoff = 80)}
mitchell.form <- bf(Dist_moved ~ scale(Day) + scale(TOD),
                family=Gamma(link='log'))
options(width=150)
mitchell.form |> get_prior(data = mitchell)
options(width=80)
```
### Defining priors

The following link provides some guidance about defining priors.
[https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations]

When defining our own priors, we typically do not want them to be scaled.

If we wanted to define our own priors that were less vague, yet still not likely
to bias the outcomes, we could try the following priors (mainly plucked out of
thin air):

- $\beta_0$: normal centred at 134 with a standard deviation of 65
  - mean of 134: since `median(log(mitchell$Dist_moved/1000))`
  - sd pf 0.7: since `mad(log(mitchell$Dist_moved/1000))`
- $\beta_1$: normal centred at 0 with a standard deviation of 2
  - sd of 1: since `mad(log(Dist_moved/1000))/mad(scale(Day))`
- $\sigma$: half-t centred at 0 with a standard deviation of 65 OR
  - sd pf 65: since `mad(log(mitchell$Dist_moved/1000))`
- $\sigma$: gamma with shape parameters of 2 and 1
- $\beta_0$: normal centred at 1.5 with a standard deviation of 1.5
  - mean of 1.5: since `mean(log(mitchell$Dist_moved/1000))` or `mean(asinh(mitchell$Dist_moved/(2*1))/log(exp(1)))`
  - sd of 1.5: since `sd(log(mitchell$Dist_moved/1000))` or `sd(asinh(mitchell$Dist_moved/(2*1))/log(exp(1)))`

```{r fitModel2h, results='markdown', eval=TRUE, mhidden=TRUE, cache=FALSE}
mitchell |> 
  summarise(
    Int_mu = median(log(Dist_moved)),
    Int_sd = mad(log(Dist_moved)),
    b1_sd = mad(log(Dist_moved))/mad(scale(Day)),
    b2_sd = mad(log(Dist_moved))/mad(scale(TOD)),
    )
```

```{r fitModel2h1, results='markdown', eval=TRUE, mhidden=TRUE}
mitchell.form <- bf(Dist_moved ~ scale(Day) + scale(TOD) + (1 | ID),
                family=Gamma(link='log'))
get_prior(mitchell.form, data = mitchell)
priors <- prior(normal(8.3, 0.8), class = 'Intercept') +
    prior(normal(0, 0.8), class = 'b') +
    prior(student_t(3, 0, 0.8), class = 'sd') +
    prior(gamma(0.01, 0.01), class = 'shape') 
mitchell.brm2 <- brm(mitchell.form, 
                 data = mitchell,
                 prior = priors,
                 sample_prior = 'only',
                 iter = 5000,
                 warmup = 1000,
                 chains = 3,  
                 cores = 3,
                 thin = 10,
                 refresh = 100,
                 seed = 123,
                 control =  list(adapt_delta = 0.99, max_treedepth = 20),
                 backend = "cmdstanr"
                 )

```

```{r partialPlot2h1a, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=6, fig.height=4}
mitchell.brm2 |>
    conditional_effects("Day") |>
    plot(points = TRUE)
mitchell.brm2 |>
    conditional_effects("Day") |>
    plot(points = TRUE) |>
    wrap_plots() &
  scale_y_log10()


mitchell.brm2 |>
    conditional_effects("TOD") |>
  plot(points = TRUE)

mitchell.brm2 |>
    conditional_effects("TOD") |>
  plot(points = TRUE) |>
    wrap_plots() &
  scale_y_log10()
```

The above seem sufficiently wide whilst at the same time not providing any encouragement for the sampler
to wander off into very unsupported areas.

```{r fitModel2h1b, results='markdown', eval=TRUE, mhidden=TRUE}
mitchell.brm3 <- update(mitchell.brm2,  
                       sample_prior = 'yes',
                       chains =  3, cores =  3,
                       refresh = 100)
save(mitchell.brm3, file = '../ws/testing/mitchell.brm3b')
```

```{r partialPlot2h1b, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=6, fig.height=4}
mitchell.brm3 |>
    conditional_effects("Day") |>
    plot(points = TRUE) |>
    wrap_plots() &
  scale_y_log10()

mitchell.brm3 |>
    conditional_effects("TOD") |>
  plot(points = TRUE) |>
    wrap_plots() &
  scale_y_log10()

mitchell.brm3 |> SUYR_prior_and_posterior()
```

::::

:::

<!-- END_PRIVATE-->

# MCMC sampling diagnostics
<!-- START_PRIVATE-->
::: {.panel-tabset}

In addition to the regular model diagnostics checking, for Bayesian analyses, it
is also necessary to explore the MCMC sampling diagnostics to be sure that the
chains are well mixed and have converged on a stable posterior.

There are a wide variety of tests that range from the big picture, overall chain
characteristics to the very specific detailed tests that allow the experienced
modeller to drill down to the very fine details of the chain behaviour.
Furthermore, there are a multitude of packages and approaches for exploring
these diagnostics.

## brms 
:::: {.panel-tabset}
### stan plots

The `brms` package offers a range of MCMC diagnostics.
Lets start with the MCMC diagnostics.

Of these, we will focus on:

- stan_trace: this plots the estimates of each parameter over the post-warmup
  length of each MCMC chain. Each chain is plotted in a different colour, with
  each parameter in its own facet. Ideally, each **trace** should just look like
  noise without any discernible drift and each of the traces for a specific
  parameter should look the same (i.e, should not be displaced above or below
  any other trace for that parameter).
  
```{r modelValidation2g, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=6, fig.height=4}
mitchell.brm3$fit |> stan_trace()
mitchell.brm3$fit |> stan_trace(inc_warmup=TRUE)
```

   The chains appear well mixed and very similar
   
- stan_acf (auto-correlation function): plots the auto-correlation between successive
  MCMC sample lags for each parameter and each chain
  
```{r modelValidation2h, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=6, fig.height=4}
mitchell.brm3$fit |> stan_ac() 
```

   There is no evidence of auto-correlation in the MCMC samples

- stan_rhat: Rhat is a **scale reduction factor** measure of convergence between the chains.  The closer the
  values are to 1, the more the chains have converged.  Values greater than 1.05
  indicate a lack of convergence.  There will be an Rhat value for each
  parameter estimated.

```{r modelValidation2i, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=6, fig.height=4}
mitchell.brm3$fit |> stan_rhat() 
```

  All Rhat values are below 1.05, suggesting the chains have converged.
  
- stan_ess (number of effective samples): the ratio of the number of effective
  samples (those not rejected by the sampler) to the number of samples provides
  an indication of the effectiveness (and efficiency) of the MCMC sampler.
  Ratios that are less than 0.5 for a parameter suggest that the sampler spent
  considerable time in difficult areas of the sampling domain and rejected more
  than half of the samples (replacing them with the previous effective sample).  
  
  If the ratios are low, tightening the priors may help.
  
```{r modelValidation2j, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=6, fig.height=4}
mitchell.brm3$fit |> stan_ess()
```

  Ratios all very high.

```{r modelValidation2k, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=6, fig.height=4}
mitchell.brm3$fit |> stan_dens(separate_chains = TRUE)
```

::::
:::
<!-- END_PRIVATE-->

# Model validation 

<!-- START_PRIVATE-->
::: {.panel-tabset}
## brms 
:::: {.panel-tabset}
### pp check
Post predictive checks provide additional diagnostics about the fit of the
model.  Specifically, they provide a comparison between predictions drawn from
the model and the observed data used to train the model.

```{r modelValidation5a, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=6, fig.height=4}
available_ppc()
```

- dens_overlay: plots the density distribution of the observed data (black line)
overlayed on top of 50 density distributions generated from draws from the model
(light blue).  Ideally, the 50 realisations should be roughly consistent with
the observed data.

```{r modelValidation5b, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=6, fig.height=4}
mitchell.brm3 |> pp_check(type = 'dens_overlay', ndraws = 100)
```

The model draws appear to represent the shape of the observed data reasonably well 

- error_scatter_avg: this plots the observed values against the average
  residuals. Similar to a residual plot, we do not want to see any patterns in
  this plot.  There is some pattern remaining in these residuals.

```{r modelValidation5c, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=6, fig.height=4}
mitchell.brm3 |> pp_check(type = 'error_scatter_avg')
```

The predictive error seems to be related to the predictor - the model performs
poorest at higher recruitments.

- intervals:  plots the observed data overlayed on top of posterior predictions
associated with each level of the predictor.  Ideally, the observed data should
all fall within the predictive intervals.


```{r modelValidation5e, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=6, fig.height=4}
mitchell.brm3 |> pp_check(type='intervals')
## mitchell.brm3 |> pp_check(group='DENSITY', type='intervals')
```

Whilst the modelled predictions do a reasonable job of representing the observed data, 
the observed data do appear to be more varied than the model is representing.

The `shinystan` package allows the full suite of MCMC diagnostics and posterior
predictive checks to be accessed via a web interface.

```{r modelValidation5g, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=6, fig.height=4}
#library(shinystan)
#launch_shinystan(mitchell.brm3)
```

### DHARMa residuals

DHARMa residuals provide very useful diagnostics.  Unfortunately, we cannot
directly use the `simulateResiduals()` function to generate the simulated
residuals.  However, if we are willing to calculate some of the components
yourself, we can still obtain the simulated residuals from the fitted stan model.

We need to supply:

- simulated (predicted) responses associated with each observation.
- observed values
- fitted (predicted) responses (averaged) associated with each observation

```{r modelValidation6a, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=8, fig.height=5}
preds <- mitchell.brm3 |> posterior_predict(nsamples = 250,  summary = FALSE)
mitchell.resids <- createDHARMa(simulatedResponse = t(preds),
                            observedResponse = mitchell$Dist,
                            fittedPredictedResponse = apply(preds, 2, median),
                            integerResponse = FALSE)
mitchell.resids |> plot()

mitchell.resids |> testDispersion()
```

```{r modelValidation6aa, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=8, fig.height=10, error=TRUE}
mitchell.resids <- make_brms_dharma_res(mitchell.brm3, integerResponse = FALSE)
wrap_elements(~testUniformity(mitchell.resids)) +
               wrap_elements(~plotResiduals(mitchell.resids, form = factor(rep(1, nrow(mitchell))))) +
               wrap_elements(~plotResiduals(mitchell.resids, quantreg = TRUE)) +
               wrap_elements(~testDispersion(mitchell.resids))

mitchell.resids |> testTemporalAutocorrelation(time = mitchell$Day)

mitchell.resid1 <- mitchell.resids |>
  recalculateResiduals(group = mitchell$Day, aggregateBy = sum)
  ## recalculateResiduals(group = interaction(mitchell$Day,  mitchell$ID),  aggregateBy = mean)
mitchell.resid1 |> testTemporalAutocorrelation(time=unique(mitchell$Day))
autocor_check(mitchell, mitchell.brm3, variable =  "Day", n.sim =  250)

mitchell.brm3 |> augment() |>
  group_by(ID) |>
  reframe(ACF = as.numeric(acf(.resid, plot =  FALSE)$acf)) |>
  group_by(ID) |>
  mutate(N =  1:n()) |> 
  ungroup() |>
  group_by(N) |>
  summarise(ACF = mean(ACF)) |> 
  ggplot(aes(y =  ACF, x =  N)) +
  geom_hline(yintercept =  0) +
  geom_segment(aes(yend =  0, xend =  N))
```
**Conclusions:**

- there is evidence of a lack of fit
- there is evidence of temporal autocorrelation

::::
:::
<!-- END_PRIVATE-->

# Refit model
 
<!-- START_PRIVATE-->
```{r fitModel4, results='markdown', eval=TRUE, mhidden=TRUE}
mitchell.form <- bf(Dist_moved ~ scale(Day) + scale(TOD) + (1 |ID) +
                      ar(time = Day, gr = ID, p = 1, cov =  TRUE),
                family=Gamma(link='log'))
get_prior(mitchell.form, data = mitchell)

priors <- prior(normal(8.3, 0.8), class = "Intercept") +
    prior(normal(0, 0.8), class = "b") +
    prior(student_t(3, 0, 0.8), class = "sd") +
    prior(gamma(0.01, 0.01), class = "shape") +
    prior(student_t(3, 0, 0.8), class = "sderr")
mitchell.brm4 <- brm(mitchell.form, 
                 data = mitchell,
                 prior = priors,
                 sample_prior = 'yes',
                 iter = 5000,
                 warmup = 1000,
                 chains = 3,  
                 cores = 3,
                 thin = 20,  ## note, I have increased this
                 refresh = 100,
                 seed = 123,
                 control =  list(adapt_delta = 0.99, max_treedepth = 20),
                 backend = "cmdstanr"
                 )

save(mitchell.brm4, file = '../ws/testing/mitchell.brm4b')
```
```{r partialPlot7h1b, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=6, fig.height=4, error=TRUE}
mitchell.brm4 |>
    conditional_effects("Day") |>
    plot(points = TRUE) |>
    wrap_plots() &
  scale_y_log10()

mitchell.brm4 |>
    conditional_effects("TOD") |>
  plot(points = TRUE) |>
    wrap_plots() &
  scale_y_log10()

mitchell.brm4 |> SUYR_prior_and_posterior()
```

### stan plots

The `brms` package offers a range of MCMC diagnostics.
Lets start with the MCMC diagnostics.

Of these, we will focus on:

- stan_trace: this plots the estimates of each parameter over the post-warmup
  length of each MCMC chain. Each chain is plotted in a different colour, with
  each parameter in its own facet. Ideally, each **trace** should just look like
  noise without any discernible drift and each of the traces for a specific
  parameter should look the same (i.e, should not be displaced above or below
  any other trace for that parameter).
  
```{r modelValidation4g, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=6, fig.height=4}
mitchell.brm4$fit |> stan_trace()
```

   The chains appear well mixed and very similar
   
- stan_acf (auto-correlation function): plots the auto-correlation between successive
  MCMC sample lags for each parameter and each chain
  
```{r modelValidation4h, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=6, fig.height=4}
mitchell.brm4$fit |> stan_ac() 
```

   There is no evidence of auto-correlation in the MCMC samples

- stan_rhat: Rhat is a **scale reduction factor** measure of convergence between the chains.  The closer the
  values are to 1, the more the chains have converged.  Values greater than 1.05
  indicate a lack of convergence.  There will be an Rhat value for each
  parameter estimated.

```{r modelValidation4i, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=6, fig.height=4}
mitchell.brm4$fit |> stan_rhat() 
```

  All Rhat values are below 1.05, suggesting the chains have converged.
  
- stan_ess (number of effective samples): the ratio of the number of effective
  samples (those not rejected by the sampler) to the number of samples provides
  an indication of the effectiveness (and efficiency) of the MCMC sampler.
  Ratios that are less than 0.5 for a parameter suggest that the sampler spent
  considerable time in difficult areas of the sampling domain and rejected more
  than half of the samples (replacing them with the previous effective sample).  
  
  If the ratios are low, tightening the priors may help.
  
```{r modelValidation4j, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=6, fig.height=4}
mitchell.brm4$fit |> stan_ess()
```

  Ratios all very high.

```{r modelValidation4k, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=6, fig.height=4}
mitchell.brm4$fit |> stan_dens(separate_chains = TRUE)
```


### pp check

```{r modelValidation6b, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=6, fig.height=4}
mitchell.brm4 |> pp_check(type = 'dens_overlay', ndraws = 100) 
```


### DHARMa residuals

DHARMa residuals provide very useful diagnostics.  Unfortunately, we cannot
directly use the `simulateResiduals()` function to generate the simulated
residuals.  However, if we are willing to calculate some of the components
yourself, we can still obtain the simulated residuals from the fitted stan model.

We need to supply:

- simulated (predicted) responses associated with each observation.
- observed values
- fitted (predicted) responses (averaged) associated with each observation


```{r modelValidation6ab, results='markdown', eval=TRUE, mhidden=TRUE, fig.width=8, fig.height=10}
mitchell.resids <- make_brms_dharma_res(mitchell.brm4, integerResponse = FALSE)
wrap_elements(~testUniformity(mitchell.resids)) +
               wrap_elements(~plotResiduals(mitchell.resids, form = factor(rep(1, nrow(mitchell))))) +
               wrap_elements(~plotResiduals(mitchell.resids, quantreg = TRUE)) +
               wrap_elements(~testDispersion(mitchell.resids))

mitchell.resid1 <- mitchell.resids |>
  recalculateResiduals(group = mitchell$Day, aggregateBy = sum)
mitchell.resid1 |> testTemporalAutocorrelation(time=unique(mitchell$Day))
autocor_check(mitchell, mitchell.brm4, variable =  "Day", n.sim =  250)
```
**Conclusions:**

- the simulated residuals do suggest that there might be a dispersion issue 
- it might be worth exploring either zero-inflation, a negative
  binomial model, or including a observation-level random effect.

```{r}
#| label: name
#| results: markup
#| eval: true
#| echo: true
#| cache: false

mitchell.brm4 |> augment() |>
  group_by(ID) |>
  reframe(ACF = as.numeric(acf(.resid, plot =  FALSE)$acf)) |>
  group_by(ID) |>
  mutate(N =  1:n()) |> 
  ungroup() |>
  group_by(N) |>
  summarise(ACF = mean(ACF)) |> 
  ggplot(aes(y =  ACF, x =  N)) +
  geom_hline(yintercept =  0) +
  geom_segment(aes(yend =  0, xend =  N))

```

<!-- END_PRIVATE-->
